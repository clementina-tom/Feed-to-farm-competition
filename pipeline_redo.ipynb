{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a380d4f",
   "metadata": {},
   "source": [
    "# Memory-Efficient ML Pipeline for Farm To Feed Dataset (Redo)\n",
    "\n",
    "This notebook implements a memory-efficient machine learning pipeline to predict customer purchasing behavior for 1-week and 2-week windows using pandas, gc, and LightGBM. Includes FileUpload widgets for easy file handling in Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ce9981c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "from sklearn.metrics import roc_auc_score, mean_absolute_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from ipywidgets import FileUpload\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73dfbfbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ba3ef028a5148c58fdc42feeacc12cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value={}, accept='.csv', description='Train.csv')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd694b55523c418092b56573850f4124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value={}, accept='.csv', description='Test.csv')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23329ffa4dca4b6ea944c88475fb29bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value={}, accept='.csv', description='customer_data.csv')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a94c88944b6543ba89f0a678c17cfc21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value={}, accept='.csv', description='sku_data.csv')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# File Upload Widgets\n",
    "upload_train = FileUpload(accept='.csv', multiple=False, description='Train.csv')\n",
    "upload_test = FileUpload(accept='.csv', multiple=False, description='Test.csv')\n",
    "upload_customer = FileUpload(accept='.csv', multiple=False, description='customer_data.csv')\n",
    "upload_sku = FileUpload(accept='.csv', multiple=False, description='sku_data.csv')\n",
    "\n",
    "display(upload_train, upload_test, upload_customer, upload_sku)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279ebab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'upload_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-938236283.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Step 1: Efficient Data Loading & Downcasting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading data...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mupload_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupload_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'upload_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 1: Efficient Data Loading & Downcasting\n",
    "print(\"Loading data...\")\n",
    "if upload_train.value:\n",
    "    train = pd.read_csv(io.BytesIO(list(upload_train.value.values())[0]['content']))\n",
    "else:\n",
    "    train = pd.read_csv('Train.csv')\n",
    "\n",
    "if upload_test.value:\n",
    "    test = pd.read_csv(io.BytesIO(list(upload_test.value.values())[0]['content']))\n",
    "else:\n",
    "    test = pd.read_csv('Test.csv')\n",
    "\n",
    "if upload_customer.value:\n",
    "    customer = pd.read_csv(io.BytesIO(list(upload_customer.value.values())[0]['content']))\n",
    "else:\n",
    "    customer = pd.read_csv('customer_data.csv')\n",
    "\n",
    "if upload_sku.value:\n",
    "    sku = pd.read_csv(io.BytesIO(list(upload_sku.value.values())[0]['content']))\n",
    "else:\n",
    "    sku = pd.read_csv('sku_data.csv')\n",
    "\n",
    "def downcast(df):\n",
    "    for col in df.select_dtypes(include=['float64']).columns:\n",
    "        df[col] = df[col].astype('float32')\n",
    "    for col in df.select_dtypes(include=['int64']).columns:\n",
    "        max_val = df[col].max()\n",
    "        if max_val < 2**8:\n",
    "            df[col] = df[col].astype('int8')\n",
    "        elif max_val < 2**16:\n",
    "            df[col] = df[col].astype('int16')\n",
    "        else:\n",
    "            df[col] = df[col].astype('int32')\n",
    "    return df\n",
    "\n",
    "train = downcast(train)\n",
    "test = downcast(test)\n",
    "customer = downcast(customer)\n",
    "sku = downcast(sku)\n",
    "\n",
    "train['week_start'] = pd.to_datetime(train['week_start'])\n",
    "test['week_start'] = pd.to_datetime(test['week_start'])\n",
    "customer['customer_created_at'] = pd.to_datetime(customer['customer_created_at'])\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa8fe05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge additional data\n",
    "train = train.merge(customer, on='customer_id', how='left', suffixes=('', '_cust'))\n",
    "train = train.merge(sku, on='product_unit_variant_id', how='left', suffixes=('', '_sku'))\n",
    "test = test.merge(customer, on='customer_id', how='left', suffixes=('', '_cust'))\n",
    "test = test.merge(sku, on='product_unit_variant_id', how='left', suffixes=('', '_sku'))\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886ccfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Smart Grid Creation & Feature Engineering\n",
    "full_df = pd.concat([train[['customer_id', 'product_unit_variant_id', 'week_start', 'qty_this_week']],\n",
    "                     test[['customer_id', 'product_unit_variant_id', 'week_start']].assign(qty_this_week=0.0)])\n",
    "full_df = downcast(full_df)\n",
    "full_df = full_df.sort_values(['customer_id', 'product_unit_variant_id', 'week_start'])\n",
    "\n",
    "print(\"Creating features...\")\n",
    "full_df['lag1_qty'] = full_df.groupby(['customer_id', 'product_unit_variant_id'])['qty_this_week'].shift(1)\n",
    "full_df['lag2_qty'] = full_df.groupby(['customer_id', 'product_unit_variant_id'])['qty_this_week'].shift(2)\n",
    "full_df['lag3_qty'] = full_df.groupby(['customer_id', 'product_unit_variant_id'])['qty_this_week'].shift(3)\n",
    "full_df['rolling_mean_4'] = full_df.groupby(['customer_id', 'product_unit_variant_id'])['qty_this_week'].rolling(4).mean().reset_index(0, drop=True)\n",
    "full_df['rolling_max_4'] = full_df.groupby(['customer_id', 'product_unit_variant_id'])['qty_this_week'].rolling(4).max().reset_index(0, drop=True)\n",
    "\n",
    "full_df[['lag1_qty', 'lag2_qty', 'lag3_qty', 'rolling_mean_4', 'rolling_max_4']] = full_df[['lag1_qty', 'lag2_qty', 'lag3_qty', 'rolling_mean_4', 'rolling_max_4']].fillna(0)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd90edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge features back\n",
    "feature_cols = ['lag1_qty', 'lag2_qty', 'lag3_qty', 'rolling_mean_4', 'rolling_max_4']\n",
    "train = train.merge(full_df[['customer_id', 'product_unit_variant_id', 'week_start'] + feature_cols], \n",
    "                    on=['customer_id', 'product_unit_variant_id', 'week_start'], how='left')\n",
    "test = test.merge(full_df[['customer_id', 'product_unit_variant_id', 'week_start'] + feature_cols], \n",
    "                  on=['customer_id', 'product_unit_variant_id', 'week_start'], how='left')\n",
    "\n",
    "del full_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fb463f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Target Generation\n",
    "print(\"Creating targets...\")\n",
    "train = train.sort_values(['customer_id', 'product_unit_variant_id', 'week_start'])\n",
    "train['target_purchase_1w'] = (train.groupby(['customer_id', 'product_unit_variant_id'])['qty_this_week'].shift(-1) > 0).astype(int)\n",
    "train['target_qty_1w'] = train.groupby(['customer_id', 'product_unit_variant_id'])['qty_this_week'].shift(-1).fillna(0)\n",
    "train['target_purchase_2w'] = (train.groupby(['customer_id', 'product_unit_variant_id'])['qty_this_week'].shift(-2) > 0).astype(int)\n",
    "train['target_qty_2w'] = train.groupby(['customer_id', 'product_unit_variant_id'])['qty_this_week'].shift(-2).fillna(0)\n",
    "\n",
    "train = train.dropna(subset=['target_purchase_1w'])\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ec0dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categoricals\n",
    "cat_cols = ['customer_category', 'customer_status', 'grade_name', 'unit_name']\n",
    "le = LabelEncoder()\n",
    "for col in cat_cols:\n",
    "    combined = pd.concat([train[col], test[col]])\n",
    "    le.fit(combined)\n",
    "    train[col] = le.transform(train[col])\n",
    "    test[col] = le.transform(test[col])\n",
    "\n",
    "features = ['selling_price', 'customer_category', 'customer_status', 'grade_name', 'unit_name'] + feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f40c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Modeling\n",
    "print(\"Training models...\")\n",
    "weeks = sorted(train['week_start'].unique())\n",
    "val_weeks = weeks[-2:]\n",
    "val_mask = train['week_start'].isin(val_weeks)\n",
    "\n",
    "models = {}\n",
    "targets = ['target_purchase_1w', 'target_qty_1w', 'target_purchase_2w', 'target_qty_2w']\n",
    "for target in targets:\n",
    "    print(f\"Training {target}...\")\n",
    "    X = train[features]\n",
    "    y = train[target]\n",
    "    X_train, X_val = X[~val_mask], X[val_mask]\n",
    "    y_train, y_val = y[~val_mask], y[val_mask]\n",
    "    \n",
    "    if 'purchase' in target:\n",
    "        model = lgb.LGBMClassifier(n_estimators=1000, early_stopping_rounds=50, verbose=-1)\n",
    "        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], eval_metric='auc')\n",
    "        pred_val = model.predict_proba(X_val)[:, 1]\n",
    "        auc = roc_auc_score(y_val, pred_val)\n",
    "        print(f\"AUC for {target}: {auc}\")\n",
    "    else:\n",
    "        model = lgb.LGBMRegressor(n_estimators=1000, early_stopping_rounds=50, verbose=-1)\n",
    "        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], eval_metric='mae')\n",
    "        pred_val = model.predict(X_val)\n",
    "        mae = mean_absolute_error(y_val, pred_val)\n",
    "        print(f\"MAE for {target}: {mae}\")\n",
    "    \n",
    "    models[target] = model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95061155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Submission\n",
    "print(\"Generating predictions...\")\n",
    "test['Target_purchase_next_1w'] = models['target_purchase_1w'].predict_proba(test[features])[:, 1]\n",
    "test['Target_qty_next_1w'] = models['target_qty_1w'].predict(test[features])\n",
    "test['Target_purchase_next_2w'] = models['target_purchase_2w'].predict_proba(test[features])[:, 1]\n",
    "test['Target_qty_next_2w'] = models['target_qty_2w'].predict(test[features])\n",
    "\n",
    "submission = test[['ID', 'Target_purchase_next_1w', 'Target_qty_next_1w', 'Target_purchase_next_2w', 'Target_qty_next_2w']]\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Submission saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
