{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2a380d4f",
      "metadata": {
        "id": "2a380d4f"
      },
      "source": [
        "# Memory-Efficient ML Pipeline for Farm To Feed Dataset (Redo)\n",
        "\n",
        "This notebook implements a memory-efficient machine learning pipeline to predict customer purchasing behavior for 1-week and 2-week windows using pandas, gc, and LightGBM. Includes FileUpload widgets for easy file handling in Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ce9981c",
      "metadata": {
        "id": "5ce9981c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "import gc\n",
        "from sklearn.metrics import roc_auc_score, mean_absolute_error\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from ipywidgets import FileUpload\n",
        "import io"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73dfbfbd",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "5ba3ef028a5148c58fdc42feeacc12cb",
            "cd694b55523c418092b56573850f4124",
            "23329ffa4dca4b6ea944c88475fb29bf",
            "a94c88944b6543ba89f0a678c17cfc21"
          ]
        },
        "id": "73dfbfbd",
        "outputId": "2b40f00f-57a7-4d14-ad3b-583a296f95ca"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5ba3ef028a5148c58fdc42feeacc12cb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "FileUpload(value={}, accept='.csv', description='Train.csv')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cd694b55523c418092b56573850f4124",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "FileUpload(value={}, accept='.csv', description='Test.csv')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "23329ffa4dca4b6ea944c88475fb29bf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "FileUpload(value={}, accept='.csv', description='customer_data.csv')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a94c88944b6543ba89f0a678c17cfc21",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "FileUpload(value={}, accept='.csv', description='sku_data.csv')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# File Upload Widgets\n",
        "upload_train = FileUpload(accept='.csv', multiple=False, description='Train.csv')\n",
        "upload_test = FileUpload(accept='.csv', multiple=False, description='Test.csv')\n",
        "upload_customer = FileUpload(accept='.csv', multiple=False, description='customer_data.csv')\n",
        "upload_sku = FileUpload(accept='.csv', multiple=False, description='sku_data.csv')\n",
        "\n",
        "display(upload_train, upload_test, upload_customer, upload_sku)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "279ebab3",
      "metadata": {
        "id": "279ebab3",
        "outputId": "3458b472-1707-4b75-d696-381a9c15e2da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'upload_train' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-938236283.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Step 1: Efficient Data Loading & Downcasting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading data...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mupload_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupload_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'upload_train' is not defined"
          ]
        }
      ],
      "source": [
        "# Step 1: Efficient Data Loading & Downcasting\n",
        "print(\"Loading data...\")\n",
        "if upload_train.value:\n",
        "    train = pd.read_csv(io.BytesIO(list(upload_train.value.values())[0]['content']))\n",
        "else:\n",
        "    train = pd.read_csv('Train.csv')\n",
        "\n",
        "if upload_test.value:\n",
        "    test = pd.read_csv(io.BytesIO(list(upload_test.value.values())[0]['content']))\n",
        "else:\n",
        "    test = pd.read_csv('Test.csv')\n",
        "\n",
        "if upload_customer.value:\n",
        "    customer = pd.read_csv(io.BytesIO(list(upload_customer.value.values())[0]['content']))\n",
        "else:\n",
        "    customer = pd.read_csv('customer_data.csv')\n",
        "\n",
        "if upload_sku.value:\n",
        "    sku = pd.read_csv(io.BytesIO(list(upload_sku.value.values())[0]['content']))\n",
        "else:\n",
        "    sku = pd.read_csv('sku_data.csv')\n",
        "\n",
        "def downcast(df):\n",
        "    for col in df.select_dtypes(include=['float64']).columns:\n",
        "        df[col] = df[col].astype('float32')\n",
        "    for col in df.select_dtypes(include=['int64']).columns:\n",
        "        max_val = df[col].max()\n",
        "        if max_val < 2**8:\n",
        "            df[col] = df[col].astype('int8')\n",
        "        elif max_val < 2**16:\n",
        "            df[col] = df[col].astype('int16')\n",
        "        else:\n",
        "            df[col] = df[col].astype('int32')\n",
        "    return df\n",
        "\n",
        "train = downcast(train)\n",
        "test = downcast(test)\n",
        "customer = downcast(customer)\n",
        "sku = downcast(sku)\n",
        "\n",
        "train['week_start'] = pd.to_datetime(train['week_start'])\n",
        "test['week_start'] = pd.to_datetime(test['week_start'])\n",
        "customer['customer_created_at'] = pd.to_datetime(customer['customer_created_at'])\n",
        "\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fa8fe05",
      "metadata": {
        "id": "4fa8fe05"
      },
      "outputs": [],
      "source": [
        "# Merge additional data\n",
        "train = train.merge(customer, on='customer_id', how='left', suffixes=('', '_cust'))\n",
        "train = train.merge(sku, on='product_unit_variant_id', how='left', suffixes=('', '_sku'))\n",
        "test = test.merge(customer, on='customer_id', how='left', suffixes=('', '_cust'))\n",
        "test = test.merge(sku, on='product_unit_variant_id', how='left', suffixes=('', '_sku'))\n",
        "\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "886ccfd0",
      "metadata": {
        "id": "886ccfd0"
      },
      "outputs": [],
      "source": [
        "# Step 2: Smart Grid Creation & Feature Engineering\n",
        "full_df = pd.concat([train[['customer_id', 'product_unit_variant_id', 'week_start', 'qty_this_week']],\n",
        "                     test[['customer_id', 'product_unit_variant_id', 'week_start']].assign(qty_this_week=0.0)])\n",
        "full_df = downcast(full_df)\n",
        "full_df = full_df.sort_values(['customer_id', 'product_unit_variant_id', 'week_start'])\n",
        "\n",
        "print(\"Creating features...\")\n",
        "full_df['lag1_qty'] = full_df.groupby(['customer_id', 'product_unit_variant_id'])['qty_this_week'].shift(1)\n",
        "full_df['lag2_qty'] = full_df.groupby(['customer_id', 'product_unit_variant_id'])['qty_this_week'].shift(2)\n",
        "full_df['lag3_qty'] = full_df.groupby(['customer_id', 'product_unit_variant_id'])['qty_this_week'].shift(3)\n",
        "full_df['rolling_mean_4'] = full_df.groupby(['customer_id', 'product_unit_variant_id'])['qty_this_week'].rolling(4).mean().reset_index(0, drop=True)\n",
        "full_df['rolling_max_4'] = full_df.groupby(['customer_id', 'product_unit_variant_id'])['qty_this_week'].rolling(4).max().reset_index(0, drop=True)\n",
        "\n",
        "full_df[['lag1_qty', 'lag2_qty', 'lag3_qty', 'rolling_mean_4', 'rolling_max_4']] = full_df[['lag1_qty', 'lag2_qty', 'lag3_qty', 'rolling_mean_4', 'rolling_max_4']].fillna(0)\n",
        "\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcd90edd",
      "metadata": {
        "id": "dcd90edd"
      },
      "outputs": [],
      "source": [
        "# Merge features back\n",
        "feature_cols = ['lag1_qty', 'lag2_qty', 'lag3_qty', 'rolling_mean_4', 'rolling_max_4']\n",
        "train = train.merge(full_df[['customer_id', 'product_unit_variant_id', 'week_start'] + feature_cols],\n",
        "                    on=['customer_id', 'product_unit_variant_id', 'week_start'], how='left')\n",
        "test = test.merge(full_df[['customer_id', 'product_unit_variant_id', 'week_start'] + feature_cols],\n",
        "                  on=['customer_id', 'product_unit_variant_id', 'week_start'], how='left')\n",
        "\n",
        "del full_df\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69fb463f",
      "metadata": {
        "id": "69fb463f"
      },
      "outputs": [],
      "source": [
        "# Step 3: Target Generation\n",
        "print(\"Creating targets...\")\n",
        "train = train.sort_values(['customer_id', 'product_unit_variant_id', 'week_start'])\n",
        "train['target_purchase_1w'] = (train.groupby(['customer_id', 'product_unit_variant_id'])['qty_this_week'].shift(-1) > 0).astype(int)\n",
        "train['target_qty_1w'] = train.groupby(['customer_id', 'product_unit_variant_id'])['qty_this_week'].shift(-1).fillna(0)\n",
        "train['target_purchase_2w'] = (train.groupby(['customer_id', 'product_unit_variant_id'])['qty_this_week'].shift(-2) > 0).astype(int)\n",
        "train['target_qty_2w'] = train.groupby(['customer_id', 'product_unit_variant_id'])['qty_this_week'].shift(-2).fillna(0)\n",
        "\n",
        "train = train.dropna(subset=['target_purchase_1w'])\n",
        "\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68ec0dbf",
      "metadata": {
        "id": "68ec0dbf"
      },
      "outputs": [],
      "source": [
        "# Encode categoricals\n",
        "cat_cols = ['customer_category', 'customer_status', 'grade_name', 'unit_name']\n",
        "le = LabelEncoder()\n",
        "for col in cat_cols:\n",
        "    combined = pd.concat([train[col], test[col]])\n",
        "    le.fit(combined)\n",
        "    train[col] = le.transform(train[col])\n",
        "    test[col] = le.transform(test[col])\n",
        "\n",
        "features = ['selling_price', 'customer_category', 'customer_status', 'grade_name', 'unit_name'] + feature_cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42f40c55",
      "metadata": {
        "id": "42f40c55"
      },
      "outputs": [],
      "source": [
        "# Step 4: Modeling\n",
        "print(\"Training models...\")\n",
        "weeks = sorted(train['week_start'].unique())\n",
        "val_weeks = weeks[-2:]\n",
        "val_mask = train['week_start'].isin(val_weeks)\n",
        "\n",
        "models = {}\n",
        "targets = ['target_purchase_1w', 'target_qty_1w', 'target_purchase_2w', 'target_qty_2w']\n",
        "for target in targets:\n",
        "    print(f\"Training {target}...\")\n",
        "    X = train[features]\n",
        "    y = train[target]\n",
        "    X_train, X_val = X[~val_mask], X[val_mask]\n",
        "    y_train, y_val = y[~val_mask], y[val_mask]\n",
        "\n",
        "    if 'purchase' in target:\n",
        "        model = lgb.LGBMClassifier(n_estimators=1000, early_stopping_rounds=50, verbose=-1)\n",
        "        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], eval_metric='auc')\n",
        "        pred_val = model.predict_proba(X_val)[:, 1]\n",
        "        auc = roc_auc_score(y_val, pred_val)\n",
        "        print(f\"AUC for {target}: {auc}\")\n",
        "    else:\n",
        "        model = lgb.LGBMRegressor(n_estimators=1000, early_stopping_rounds=50, verbose=-1)\n",
        "        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], eval_metric='mae')\n",
        "        pred_val = model.predict(X_val)\n",
        "        mae = mean_absolute_error(y_val, pred_val)\n",
        "        print(f\"MAE for {target}: {mae}\")\n",
        "\n",
        "    models[target] = model\n",
        "    gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95061155",
      "metadata": {
        "id": "95061155"
      },
      "outputs": [],
      "source": [
        "# Step 5: Submission\n",
        "print(\"Generating predictions...\")\n",
        "test['Target_purchase_next_1w'] = models['target_purchase_1w'].predict_proba(test[features])[:, 1]\n",
        "test['Target_qty_next_1w'] = models['target_qty_1w'].predict(test[features])\n",
        "test['Target_purchase_next_2w'] = models['target_purchase_2w'].predict_proba(test[features])[:, 1]\n",
        "test['Target_qty_next_2w'] = models['target_qty_2w'].predict(test[features])\n",
        "\n",
        "submission = test[['ID', 'Target_purchase_next_1w', 'Target_qty_next_1w', 'Target_purchase_next_2w', 'Target_qty_next_2w']]\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "print(\"Submission saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JlFp5PP0IidY"
      },
      "id": "JlFp5PP0IidY"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "import gc\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# --- 1. Load Data ---\n",
        "print(\"Loading...\")\n",
        "train = pd.read_csv('Train.csv')\n",
        "test = pd.read_csv('Test.csv')\n",
        "customer = pd.read_csv('customer_data.csv') # Load customer data\n",
        "sku = pd.read_csv('sku_data.csv')\n",
        "\n",
        "# --- 2. Downcast & Date Conversion ---\n",
        "def downcast(df):\n",
        "    for col in df.select_dtypes(include=['float64']).columns:\n",
        "        df[col] = df[col].astype('float32')\n",
        "    for col in df.select_dtypes(include=['int64']).columns:\n",
        "        # Adjusted int downcasting to avoid overflow for larger values\n",
        "        max_val = df[col].max()\n",
        "        min_val = df[col].min()\n",
        "        if min_val >= np.iinfo('int8').min and max_val <= np.iinfo('int8').max:\n",
        "            df[col] = df[col].astype('int8')\n",
        "        elif min_val >= np.iinfo('int16').min and max_val <= np.iinfo('int16').max:\n",
        "            df[col] = df[col].astype('int16')\n",
        "        elif min_val >= np.iinfo('int32').min and max_val <= np.iinfo('int32').max:\n",
        "            df[col] = df[col].astype('int32')\n",
        "    return df\n",
        "\n",
        "train = downcast(train)\n",
        "test = downcast(test)\n",
        "customer = downcast(customer) # Downcast customer data\n",
        "sku = downcast(sku)\n",
        "\n",
        "train['week_start'] = pd.to_datetime(train['week_start'])\n",
        "test['week_start'] = pd.to_datetime(test['week_start'])\n",
        "customer['customer_created_at'] = pd.to_datetime(customer['customer_created_at'])\n",
        "\n",
        "# --- Merge additional data (customer and sku) into train and test FIRST ---\n",
        "# This ensures all categorical and customer_created_at columns are present in train/test\n",
        "train = train.merge(customer, on='customer_id', how='left')\n",
        "train = train.merge(sku, on='product_unit_variant_id', how='left')\n",
        "test = test.merge(customer, on='customer_id', how='left')\n",
        "test = test.merge(sku, on='product_unit_variant_id', how='left')\n",
        "\n",
        "print(\"Train columns after merges:\", train.columns.tolist())\n",
        "print(\"Test columns after merges:\", test.columns.tolist())\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "# --- 3. Feature Engineering (Unified) ---\n",
        "# Combine Train and Test to generate Lag Features smoothly\n",
        "# Drop 'ID' column before concat as it's not a feature and could be duplicated.\n",
        "test_with_qty = test.assign(qty_this_week=0.0).drop(columns=['ID'], errors='ignore')\n",
        "full_df = pd.concat([train.drop(columns=['ID'], errors='ignore'), test_with_qty], ignore_index=True)\n",
        "\n",
        "# The merges with customer and sku are now done earlier on train/test, no need to merge into full_df again here.\n",
        "\n",
        "full_df = downcast(full_df) # Downcast full_df after concat\n",
        "full_df = full_df.sort_values(['customer_id', 'product_unit_variant_id', 'week_start'])\n",
        "\n",
        "print(\"Full_df columns after concat and sort:\", full_df.columns.tolist())\n",
        "\n",
        "# Generate Lags\n",
        "print(\"Generating features...\")\n",
        "grp = full_df.groupby(['customer_id', 'product_unit_variant_id'])['qty_this_week']\n",
        "full_df['lag1'] = grp.shift(1)\n",
        "full_df['lag2'] = grp.shift(2)\n",
        "full_df['roll_mean_4'] = grp.rolling(4).mean().reset_index(level=[0,1], drop=True)\n",
        "\n",
        "# Fill NaNs\n",
        "full_df[['lag1', 'lag2', 'roll_mean_4']] = full_df[['lag1', 'lag2', 'roll_mean_4']].fillna(0)\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "# --- 4. Create \"Stacked\" Training Data (The Grandmaster Move) ---\n",
        "# We transform the data:\n",
        "# Row 1: Target=Week1, Horizon=1\n",
        "# Row 2: Target=Week2, Horizon=2\n",
        "\n",
        "# Ensure all relevant columns are carried over from full_df\n",
        "# Use all columns that will be features or used for stacking\n",
        "relevant_cols = ['customer_id', 'product_unit_variant_id', 'week_start', 'qty_this_week',\n",
        "                 'customer_category_x', 'customer_status_x', 'grade_name_x', 'unit_name_x',\n",
        "                 'lag1', 'lag2', 'roll_mean_4', 'customer_created_at_x'] # Use _x suffixes for these columns\n",
        "\n",
        "print(\"Full_df columns before train_df/test_df split:\", full_df.columns.tolist())\n",
        "\n",
        "train_df = full_df[full_df['week_start'].isin(train['week_start'])][relevant_cols].copy()\n",
        "test_df = full_df[full_df['week_start'].isin(test['week_start'])][relevant_cols].copy()\n",
        "\n",
        "# Create Horizon 1 Subset\n",
        "h1 = train_df.copy()\n",
        "h1['target_qty'] = h1.groupby(['customer_id', 'product_unit_variant_id'])['qty_this_week'].shift(-1)\n",
        "h1['target_buy'] = (h1['target_qty'] > 0).astype(int)\n",
        "h1['horizon'] = 1\n",
        "\n",
        "# Create Horizon 2 Subset\n",
        "h2 = train_df.copy()\n",
        "h2['target_qty'] = h2.groupby(['customer_id', 'product_unit_variant_id'])['qty_this_week'].shift(-2)\n",
        "h2['target_buy'] = (h2['target_qty'] > 0).astype(int)\n",
        "h2['horizon'] = 2\n",
        "\n",
        "# Stack them!\n",
        "train_stacked = pd.concat([h1, h2], ignore_index=True)\n",
        "train_stacked = train_stacked.dropna(subset=['target_qty']) # Remove rows where target is NaN (end of data)\n",
        "\n",
        "# --- 5. Train Unified Models ---\n",
        "# Add the new merged features to the list of features, including customer_category and customer_status\n",
        "features = ['lag1', 'lag2', 'roll_mean_4', 'horizon',\n",
        "            'customer_category_x', 'customer_status_x', 'grade_name_x', 'unit_name_x'] # Use _x suffixes\n",
        "\n",
        "# Before training, we need to LabelEncode the categorical features\n",
        "cat_cols_to_encode = ['customer_category_x', 'customer_status_x', 'grade_name_x', 'unit_name_x'] # Use _x suffixes\n",
        "le = LabelEncoder()\n",
        "for col in cat_cols_to_encode:\n",
        "    # Fit on combined data from train_stacked, test_h1, test_h2 for consistent encoding\n",
        "    # Note: test_df is used for encoding consistency across train_stacked and final test predictions\n",
        "    combined_data = pd.concat([train_stacked[col], test_df[col]]).astype(str).fillna('UNKNOWN')\n",
        "    le.fit(combined_data)\n",
        "\n",
        "    train_stacked[col] = le.transform(train_stacked[col].astype(str).fillna('UNKNOWN'))\n",
        "    test_df[col] = le.transform(test_df[col].astype(str).fillna('UNKNOWN'))\n",
        "\n",
        "# Prepare Test Set for H1 and H2 BEFORE encoding customer data to ensure test_df is used correctly\n",
        "test_h1 = test_df.copy()\n",
        "test_h2 = test_df.copy()\n",
        "\n",
        "# Apply encoding to test_h1 and test_h2 based on the encoded test_df (already done above, but confirm this line is still needed for clarity if test_h1/h2 were not direct copies)\n",
        "# This line ensures test_h1 and test_h2 get the encoded values from test_df\n",
        "test_h1[cat_cols_to_encode] = test_df[cat_cols_to_encode]\n",
        "test_h2[cat_cols_to_encode] = test_df[cat_cols_to_encode]\n",
        "\n",
        "print(\"Training Unified Classifier...\")\n",
        "clf = lgb.LGBMClassifier(n_estimators=1500, learning_rate=0.05)\n",
        "clf.fit(train_stacked[features], train_stacked['target_buy'])\n",
        "\n",
        "print(\"Training Unified Regressor (Tweedie)...\")\n",
        "reg = lgb.LGBMRegressor(objective='tweedie', n_estimators=1500, learning_rate=0.05)\n",
        "reg.fit(train_stacked[features], train_stacked['target_qty'])\n",
        "\n",
        "# --- 6. Predict ---\n",
        "test_h1['horizon'] = 1\n",
        "test_h2['horizon'] = 2\n",
        "\n",
        "submission = test[['ID']].copy()\n",
        "submission['Target_purchase_next_1w'] = clf.predict_proba(test_h1[features])[:, 1]\n",
        "submission['Target_qty_next_1w'] = reg.predict(test_h1[features])\n",
        "submission['Target_purchase_next_2w'] = clf.predict_proba(test_h2[features])[:, 1]\n",
        "submission['Target_qty_next_2w'] = reg.predict(test_h2[features])\n",
        "\n",
        "submission.to_csv('submission_horizon.csv', index=False)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CMyzMUIIeld",
        "outputId": "9bdf3e08-b70a-432d-dce9-4bf903ea3de2"
      },
      "id": "6CMyzMUIIeld",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading...\n",
            "Train columns after merges: ['ID', 'customer_id', 'product_unit_variant_id', 'week_start', 'qty_this_week', 'num_orders_week', 'spend_this_week', 'purchased_this_week', 'product_id', 'grade_name_x', 'unit_name_x', 'product_grade_variant_id', 'selling_price', 'customer_category_x', 'customer_status_x', 'customer_created_at_x', 'Target_qty_next_1w', 'Target_purchase_next_1w', 'Target_qty_next_2w', 'Target_purchase_next_2w', 'customer_category_y', 'customer_status_y', 'customer_created_at_y', 'product_name', 'product_grade_variant_sku', 'unit_name_y', 'grade_name_y', 'grade_active_status']\n",
            "Test columns after merges: ['ID', 'customer_id', 'product_unit_variant_id', 'week_start', 'product_id', 'grade_name_x', 'unit_name_x', 'product_grade_variant_id', 'customer_category_x', 'customer_status_x', 'customer_created_at_x', 'customer_category_y', 'customer_status_y', 'customer_created_at_y', 'product_name', 'product_grade_variant_sku', 'unit_name_y', 'grade_name_y', 'grade_active_status']\n",
            "Full_df columns after concat and sort: ['customer_id', 'product_unit_variant_id', 'week_start', 'qty_this_week', 'num_orders_week', 'spend_this_week', 'purchased_this_week', 'product_id', 'grade_name_x', 'unit_name_x', 'product_grade_variant_id', 'selling_price', 'customer_category_x', 'customer_status_x', 'customer_created_at_x', 'Target_qty_next_1w', 'Target_purchase_next_1w', 'Target_qty_next_2w', 'Target_purchase_next_2w', 'customer_category_y', 'customer_status_y', 'customer_created_at_y', 'product_name', 'product_grade_variant_sku', 'unit_name_y', 'grade_name_y', 'grade_active_status']\n",
            "Generating features...\n",
            "Full_df columns before train_df/test_df split: ['customer_id', 'product_unit_variant_id', 'week_start', 'qty_this_week', 'num_orders_week', 'spend_this_week', 'purchased_this_week', 'product_id', 'grade_name_x', 'unit_name_x', 'product_grade_variant_id', 'selling_price', 'customer_category_x', 'customer_status_x', 'customer_created_at_x', 'Target_qty_next_1w', 'Target_purchase_next_1w', 'Target_qty_next_2w', 'Target_purchase_next_2w', 'customer_category_y', 'customer_status_y', 'customer_created_at_y', 'product_name', 'product_grade_variant_sku', 'unit_name_y', 'grade_name_y', 'grade_active_status', 'lag1', 'lag2', 'roll_mean_4']\n",
            "Training Unified Classifier...\n",
            "Training Unified Regressor (Tweedie)...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "import gc\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# --- 1. Load Data ---\n",
        "print(\"Loading data...\")\n",
        "train = pd.read_csv('Train.csv')\n",
        "test = pd.read_csv('Test.csv')\n",
        "customer = pd.read_csv('customer_data.csv')\n",
        "sku = pd.read_csv('sku_data.csv')\n",
        "\n",
        "# --- 2. Downcast & Pre-processing ---\n",
        "def downcast(df):\n",
        "    for col in df.select_dtypes(include=['float64']).columns:\n",
        "        df[col] = df[col].astype('float32')\n",
        "    for col in df.select_dtypes(include=['int64']).columns:\n",
        "        if df[col].max() < 2**32:\n",
        "            df[col] = df[col].astype('int32')\n",
        "    return df\n",
        "\n",
        "train = downcast(train)\n",
        "test = downcast(test)\n",
        "customer = downcast(customer)\n",
        "sku = downcast(sku)\n",
        "\n",
        "# Convert Dates\n",
        "train['week_start'] = pd.to_datetime(train['week_start'])\n",
        "test['week_start'] = pd.to_datetime(test['week_start'])\n",
        "customer['customer_created_at'] = pd.to_datetime(customer['customer_created_at'])\n",
        "\n",
        "# --- 3. Unified Feature Engineering (The \"Full Grid\" Approach) ---\n",
        "print(\"Building Full History...\")\n",
        "\n",
        "# A. Combine Train and Test FIRST\n",
        "# We need the Test set to exist so that 'shift(-1)' on the last week of Train\n",
        "# can actually see the first week of Test (if applicable) or handle boundaries correctly.\n",
        "full_df = pd.concat([\n",
        "    train[['customer_id', 'product_unit_variant_id', 'week_start', 'qty_this_week']],\n",
        "    test[['customer_id', 'product_unit_variant_id', 'week_start']].assign(qty_this_week=0.0)\n",
        "], ignore_index=True)\n",
        "\n",
        "full_df = full_df.sort_values(['customer_id', 'product_unit_variant_id', 'week_start'])\n",
        "\n",
        "# B. Add \"Grandmaster Features\" (Seasonality & Trends)\n",
        "# Fix: Handle potential NaT values before converting to int\n",
        "full_df['month'] = full_df['week_start'].dt.month.fillna(0).astype(int)\n",
        "full_df['week_of_year'] = full_df['week_start'].dt.isocalendar().week.fillna(0).astype(int)\n",
        "\n",
        "# C. Global Product Trend (Shifted to prevent leakage)\n",
        "# \"How much is this product selling globally across all customers?\"\n",
        "prod_trend = full_df.groupby(['product_unit_variant_id', 'week_start'])['qty_this_week'].mean().reset_index().rename(columns={'qty_this_week': 'global_qty'})\n",
        "prod_trend = prod_trend.sort_values(['product_unit_variant_id', 'week_start'])\n",
        "prod_trend['global_trend_lag1'] = prod_trend.groupby('product_unit_variant_id')['global_qty'].shift(1)\n",
        "full_df = full_df.merge(prod_trend[['product_unit_variant_id', 'week_start', 'global_trend_lag1']],\n",
        "                        on=['product_unit_variant_id', 'week_start'], how='left')\n",
        "\n",
        "# D. Lag Features\n",
        "print(\"Generating lags...\")\n",
        "grp = full_df.groupby(['customer_id', 'product_unit_variant_id'])['qty_this_week']\n",
        "full_df['lag1'] = grp.shift(1)\n",
        "full_df['lag2'] = grp.shift(2)\n",
        "full_df['roll_mean_4'] = grp.rolling(4).mean().reset_index(level=[0,1], drop=True)\n",
        "\n",
        "# --- 4. Target Generation (CRITICAL FIX) ---\n",
        "# We generate targets ON THE FULL DATASET so the shifts work correctly.\n",
        "print(\"Generating Targets on Full Data...\")\n",
        "full_df['target_qty_1w'] = grp.shift(-1) # Next week's qty\n",
        "full_df['target_qty_2w'] = grp.shift(-2) # Week after next qty\n",
        "\n",
        "# --- 5. Merge Metadata ---\n",
        "# Now we bring in Customer and SKU details\n",
        "full_df = full_df.merge(customer, on='customer_id', how='left')\n",
        "full_df = full_df.merge(sku, on='product_unit_variant_id', how='left')\n",
        "\n",
        "# Encode Categoricals\n",
        "cat_cols = ['customer_category', 'customer_status', 'grade_name', 'unit_name']\n",
        "le = LabelEncoder()\n",
        "for col in cat_cols:\n",
        "    full_df[col] = full_df[col].astype(str).fillna('UNKNOWN')\n",
        "    full_df[col] = le.fit_transform(full_df[col])\n",
        "\n",
        "# Fill Numerical NaNs\n",
        "num_cols = ['lag1', 'lag2', 'roll_mean_4', 'global_trend_lag1']\n",
        "full_df[num_cols] = full_df[num_cols].fillna(0)\n",
        "\n",
        "# --- 6. Create Stacked Training Data ---\n",
        "print(\"Stacking Data...\")\n",
        "\n",
        "# Identify Train rows vs Test rows\n",
        "# Train rows are those that were in the original Train CSV\n",
        "train_mask = full_df['week_start'].isin(train['week_start'].unique())\n",
        "train_df_source = full_df[train_mask].copy()\n",
        "test_df_source = full_df[~train_mask].copy()\n",
        "\n",
        "# Stack 1: Horizon 1\n",
        "h1 = train_df_source.copy()\n",
        "h1['target_qty'] = h1['target_qty_1w']\n",
        "h1['target_buy'] = (h1['target_qty'] > 0).astype(int)\n",
        "h1['horizon'] = 1\n",
        "\n",
        "# Stack 2: Horizon 2\n",
        "h2 = train_df_source.copy()\n",
        "h2['target_qty'] = h2['target_qty_2w']\n",
        "h2['target_buy'] = (h2['target_qty'] > 0).astype(int)\n",
        "h2['horizon'] = 2\n",
        "\n",
        "# Combine\n",
        "train_stacked = pd.concat([h1, h2], ignore_index=True)\n",
        "\n",
        "# Drop rows where target is NaN (End of history)\n",
        "train_stacked = train_stacked.dropna(subset=['target_qty'])\n",
        "\n",
        "# Define Features\n",
        "features = ['lag1', 'lag2', 'roll_mean_4', 'global_trend_lag1',\n",
        "            'month', 'week_of_year', 'horizon',\n",
        "            'customer_category', 'customer_status', 'grade_name', 'unit_name']\n",
        "\n",
        "# --- 7. Modeling ---\n",
        "print(\"Training Unified Models...\")\n",
        "\n",
        "# Classifier (AUC)\n",
        "clf = lgb.LGBMClassifier(n_estimators=2000, learning_rate=0.03, num_leaves=31, random_state=42)\n",
        "clf.fit(train_stacked[features], train_stacked['target_buy'])\n",
        "\n",
        "# Regressor (Tweedie for MAE)\n",
        "reg = lgb.LGBMRegressor(objective='tweedie', tweedie_variance_power=1.5,\n",
        "                        n_estimators=2000, learning_rate=0.03, num_leaves=31, random_state=42)\n",
        "reg.fit(train_stacked[features], train_stacked['target_qty'])\n",
        "\n",
        "# --- 8. Prediction ---\n",
        "print(\"Generating Predictions...\")\n",
        "\n",
        "# Prepare Test sets\n",
        "test_h1 = test_df_source.copy()\n",
        "test_h1['horizon'] = 1\n",
        "\n",
        "test_h2 = test_df_source.copy()\n",
        "test_h2['horizon'] = 2\n",
        "\n",
        "# Predict\n",
        "submission = test[['ID']].copy()\n",
        "submission['Target_purchase_next_1w'] = clf.predict_proba(test_h1[features])[:, 1]\n",
        "submission['Target_qty_next_1w'] = reg.predict(test_h1[features])\n",
        "submission['Target_purchase_next_2w'] = clf.predict_proba(test_h2[features])[:, 1]\n",
        "submission['Target_qty_next_2w'] = reg.predict(test_h2[features])\n",
        "\n",
        "# --- 9. Consistency Check ---\n",
        "# If probability of buy is very low, force quantity to 0?\n",
        "# (Optional, but helps MAE). Let's be conservative.\n",
        "# submission.loc[submission['Target_purchase_next_1w'] < 0.1, 'Target_qty_next_1w'] = 0\n",
        "# submission.loc[submission['Target_purchase_next_2w'] < 0.1, 'Target_qty_next_2w'] = 0\n",
        "\n",
        "submission.to_csv('submission_horizon_fixed.csv', index=False)\n",
        "print(\"Done! Saved as submission_horizon_fixed.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTxCsTJ4pfoX",
        "outputId": "a8aa24bc-81e2-4b31-bf50-69e87045cd71"
      },
      "id": "MTxCsTJ4pfoX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Building Full History...\n",
            "Generating lags...\n",
            "Generating Targets on Full Data...\n",
            "Stacking Data...\n",
            "Training Unified Models...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "import gc\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# --- 1. Load Data ---\n",
        "print(\"Loading data...\")\n",
        "train = pd.read_csv('Train.csv')\n",
        "test = pd.read_csv('Test.csv')\n",
        "customer = pd.read_csv('customer_data.csv')\n",
        "sku = pd.read_csv('sku_data.csv')\n",
        "\n",
        "# --- 2. Downcast to Save Memory ---\n",
        "def downcast(df):\n",
        "    for col in df.select_dtypes(include=['float64']).columns:\n",
        "        df[col] = df[col].astype('float32')\n",
        "    for col in df.select_dtypes(include=['int64']).columns:\n",
        "        if df[col].max() < 2**32:\n",
        "            df[col] = df[col].astype('int32')\n",
        "    return df\n",
        "\n",
        "train = downcast(train)\n",
        "test = downcast(test)\n",
        "customer = downcast(customer)\n",
        "sku = downcast(sku)\n",
        "\n",
        "# Convert Dates\n",
        "train['week_start'] = pd.to_datetime(train['week_start'])\n",
        "test['week_start'] = pd.to_datetime(test['week_start'])\n",
        "customer['customer_created_at'] = pd.to_datetime(customer['customer_created_at'])\n",
        "\n",
        "# --- 3. Unified Feature Engineering ---\n",
        "print(\"Building Full History...\")\n",
        "\n",
        "# A. Combine Train and Test FIRST\n",
        "# We need the full timeline to calculate trends and lags correctly\n",
        "full_df = pd.concat([\n",
        "    train[['customer_id', 'product_unit_variant_id', 'week_start', 'qty_this_week']],\n",
        "    test[['customer_id', 'product_unit_variant_id', 'week_start']].assign(qty_this_week=0.0)\n",
        "], ignore_index=True)\n",
        "\n",
        "full_df = full_df.sort_values(['customer_id', 'product_unit_variant_id', 'week_start'])\n",
        "\n",
        "# --- GRANDMASTER FEATURES ---\n",
        "\n",
        "# 1. Seasonality (Time)\n",
        "full_df['month'] = full_df['week_start'].dt.month.fillna(0).astype(int)\n",
        "full_df['week_of_year'] = full_df['week_start'].dt.isocalendar().week.fillna(0).astype(int)\n",
        "\n",
        "# 2. Global Product Trend (The \"Viral\" Factor)\n",
        "# Calculates: \"How much is this specific product selling across ALL customers right now?\"\n",
        "# We shift by 1 week so we don't peek into the future.\n",
        "prod_trend = full_df.groupby(['product_unit_variant_id', 'week_start'])['qty_this_week'].mean().reset_index().rename(columns={'qty_this_week': 'global_qty'})\n",
        "prod_trend = prod_trend.sort_values(['product_unit_variant_id', 'week_start'])\n",
        "prod_trend['global_trend_lag1'] = prod_trend.groupby('product_unit_variant_id')['global_qty'].shift(1)\n",
        "prod_trend['global_trend_roll4'] = prod_trend.groupby('product_unit_variant_id')['global_qty'].transform(lambda x: x.shift(1).rolling(4).mean())\n",
        "\n",
        "full_df = full_df.merge(prod_trend[['product_unit_variant_id', 'week_start', 'global_trend_lag1', 'global_trend_roll4']],\n",
        "                        on=['product_unit_variant_id', 'week_start'], how='left')\n",
        "\n",
        "# 3. Lag Features (Individual History)\n",
        "print(\"Generating lags...\")\n",
        "grp = full_df.groupby(['customer_id', 'product_unit_variant_id'])['qty_this_week']\n",
        "full_df['lag1'] = grp.shift(1)\n",
        "full_df['lag2'] = grp.shift(2)\n",
        "full_df['lag3'] = grp.shift(3)\n",
        "full_df['roll_mean_4'] = grp.rolling(4).mean().reset_index(level=[0,1], drop=True)\n",
        "full_df['roll_max_4'] = grp.rolling(4).max().reset_index(level=[0,1], drop=True)\n",
        "\n",
        "# --- 4. Target Generation (THE FIX) ---\n",
        "# Generate targets on the full timeline BEFORE splitting\n",
        "print(\"Generating Targets...\")\n",
        "full_df['target_qty_1w'] = grp.shift(-1) # Next week's qty\n",
        "full_df['target_qty_2w'] = grp.shift(-2) # Week after next qty\n",
        "\n",
        "# --- 5. Merge Metadata & Encode ---\n",
        "full_df = full_df.merge(customer, on='customer_id', how='left')\n",
        "full_df = full_df.merge(sku, on='product_unit_variant_id', how='left')\n",
        "\n",
        "# Customer Tenure (How long have they been with us?)\n",
        "full_df['customer_tenure_days'] = (full_df['week_start'] - full_df['customer_created_at']).dt.days\n",
        "\n",
        "# Encode Categoricals\n",
        "cat_cols = ['customer_category', 'customer_status', 'grade_name', 'unit_name']\n",
        "le = LabelEncoder()\n",
        "for col in cat_cols:\n",
        "    full_df[col] = full_df[col].astype(str).fillna('UNKNOWN')\n",
        "    full_df[col] = le.fit_transform(full_df[col])\n",
        "\n",
        "# Fill NaNs in features with 0\n",
        "num_cols = ['lag1', 'lag2', 'lag3', 'roll_mean_4', 'roll_max_4', 'global_trend_lag1', 'global_trend_roll4', 'customer_tenure_days']\n",
        "full_df[num_cols] = full_df[num_cols].fillna(0)\n",
        "\n",
        "# --- 6. Create Stacked Training Data ---\n",
        "print(\"Stacking Data...\")\n",
        "\n",
        "# Filter back to Train and Test sets\n",
        "train_mask = full_df['week_start'].isin(train['week_start'].unique())\n",
        "train_df_source = full_df[train_mask].copy()\n",
        "test_df_source = full_df[~train_mask].copy()\n",
        "\n",
        "# Stack 1: Horizon 1 (Predicting Next Week)\n",
        "h1 = train_df_source.copy()\n",
        "h1['target_qty'] = h1['target_qty_1w']\n",
        "h1['target_buy'] = (h1['target_qty'] > 0).astype(int)\n",
        "h1['horizon'] = 1\n",
        "\n",
        "# Stack 2: Horizon 2 (Predicting 2 Weeks out)\n",
        "h2 = train_df_source.copy()\n",
        "h2['target_qty'] = h2['target_qty_2w']\n",
        "h2['target_buy'] = (h2['target_qty'] > 0).astype(int)\n",
        "h2['horizon'] = 2\n",
        "\n",
        "# Combine Stacks\n",
        "train_stacked = pd.concat([h1, h2], ignore_index=True)\n",
        "train_stacked = train_stacked.dropna(subset=['target_qty']) # Valid targets only\n",
        "\n",
        "# Define Features\n",
        "features = [\n",
        "    'lag1', 'lag2', 'lag3', 'roll_mean_4', 'roll_max_4', # Individual behavior\n",
        "    'global_trend_lag1', 'global_trend_roll4',           # Market behavior\n",
        "    'month', 'week_of_year', 'horizon',                  # Time context\n",
        "    'customer_tenure_days',                              # Loyalty\n",
        "    'customer_category', 'customer_status', 'grade_name', 'unit_name' # Metadata\n",
        "]\n",
        "\n",
        "# --- 7. Modeling (High Precision) ---\n",
        "print(\"Training Unified Models...\")\n",
        "\n",
        "# Classifier (Optimized for AUC)\n",
        "# Lower learning rate (0.02) + More trees (2500) = Better Pattern Recognition\n",
        "clf = lgb.LGBMClassifier(\n",
        "    n_estimators=2500,\n",
        "    learning_rate=0.02,\n",
        "    num_leaves=40,\n",
        "    random_state=42,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8\n",
        ")\n",
        "clf.fit(train_stacked[features], train_stacked['target_buy'])\n",
        "print(\"Classifier Trained.\")\n",
        "\n",
        "# Regressor (Optimized for Tweedie/MAE)\n",
        "reg = lgb.LGBMRegressor(\n",
        "    objective='tweedie',\n",
        "    tweedie_variance_power=1.5,\n",
        "    n_estimators=2500,\n",
        "    learning_rate=0.02,\n",
        "    num_leaves=40,\n",
        "    random_state=42,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8\n",
        ")\n",
        "reg.fit(train_stacked[features], train_stacked['target_qty'])\n",
        "print(\"Regressor Trained.\")\n",
        "\n",
        "# --- 8. Prediction ---\n",
        "print(\"Generating Predictions...\")\n",
        "\n",
        "# Prepare Test sets for both horizons\n",
        "test_h1 = test_df_source.copy()\n",
        "test_h1['horizon'] = 1\n",
        "\n",
        "test_h2 = test_df_source.copy()\n",
        "test_h2['horizon'] = 2\n",
        "\n",
        "submission = test[['ID']].copy()\n",
        "\n",
        "# Horizon 1 Predictions\n",
        "submission['Target_purchase_next_1w'] = clf.predict_proba(test_h1[features])[:, 1]\n",
        "submission['Target_qty_next_1w'] = reg.predict(test_h1[features])\n",
        "\n",
        "# Horizon 2 Predictions\n",
        "submission['Target_purchase_next_2w'] = clf.predict_proba(test_h2[features])[:, 1]\n",
        "submission['Target_qty_next_2w'] = reg.predict(test_h2[features])\n",
        "\n",
        "# Final Cleanup (No negative quantities)\n",
        "submission['Target_qty_next_1w'] = submission['Target_qty_next_1w'].clip(lower=0)\n",
        "submission['Target_qty_next_2w'] = submission['Target_qty_next_2w'].clip(lower=0)\n",
        "\n",
        "submission.to_csv('submission_horizon_promax.csv', index=False)\n",
        "print(\"Done! Saved as submission_horizon_promax.csv\")"
      ],
      "metadata": {
        "id": "aBPu2wyiqCkM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "560c5ddf-badd-4ef8-a01a-69acbd1d35a8"
      },
      "id": "aBPu2wyiqCkM",
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Building Full History...\n",
            "Generating lags...\n",
            "Generating Targets...\n",
            "Stacking Data...\n",
            "Training Unified Models...\n",
            "Classifier Trained.\n",
            "Regressor Trained.\n",
            "Generating Predictions...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 7. Validation & Modeling (With Scores!) ---\n",
        "print(\"--- Starting Validation ---\")\n",
        "\n",
        "# A. Create a Time-Based Split for Validation\n",
        "# We use the last 4 weeks of the training data to check our score\n",
        "weeks = sorted(train_stacked['week_start'].unique())\n",
        "val_start_week = weeks[-4]\n",
        "\n",
        "train_subset = train_stacked[train_stacked['week_start'] < val_start_week]\n",
        "val_subset = train_stacked[train_stacked['week_start'] >= val_start_week]\n",
        "\n",
        "# B. Train & Score Classifier (AUC)\n",
        "print(\"Validating Classifier...\")\n",
        "clf_val = lgb.LGBMClassifier(\n",
        "    n_estimators=2000,\n",
        "    learning_rate=0.02,\n",
        "    num_leaves=40,\n",
        "    random_state=42,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8\n",
        ")\n",
        "clf_val.fit(\n",
        "    train_subset[features],\n",
        "    train_subset['target_buy'],\n",
        "    eval_set=[(val_subset[features], val_subset['target_buy'])],\n",
        "    eval_metric='auc',\n",
        "    callbacks=[lgb.early_stopping(stopping_rounds=100)]\n",
        ")\n",
        "\n",
        "# Calculate Validation AUC\n",
        "val_preds_prob = clf_val.predict_proba(val_subset[features])[:, 1]\n",
        "from sklearn.metrics import roc_auc_score, mean_absolute_error\n",
        "val_auc = roc_auc_score(val_subset['target_buy'], val_preds_prob)\n",
        "print(f\"✅ LOCAL VALIDATION AUC: {val_auc:.5f}\")\n",
        "\n",
        "# C. Train & Score Regressor (MAE)\n",
        "print(\"Validating Regressor...\")\n",
        "reg_val = lgb.LGBMRegressor(\n",
        "    objective='tweedie',\n",
        "    tweedie_variance_power=1.5,\n",
        "    n_estimators=2000,\n",
        "    learning_rate=0.02,\n",
        "    num_leaves=40,\n",
        "    random_state=42,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8\n",
        ")\n",
        "reg_val.fit(\n",
        "    train_subset[features],\n",
        "    train_subset['target_qty'],\n",
        "    eval_set=[(val_subset[features], val_subset['target_qty'])],\n",
        "    eval_metric='mae',\n",
        "    callbacks=[lgb.early_stopping(stopping_rounds=100)]\n",
        ")\n",
        "\n",
        "# Calculate Validation MAE\n",
        "val_preds_qty = reg_val.predict(val_subset[features])\n",
        "val_mae = mean_absolute_error(val_subset['target_qty'], val_preds_qty)\n",
        "print(f\"✅ LOCAL VALIDATION MAE: {val_mae:.5f}\")\n",
        "\n",
        "# --- 8. Final Retraining & Prediction ---\n",
        "print(\"\\n--- Retraining on FULL Dataset for Submission ---\")\n",
        "# Now that we know the score, we train on EVERYTHING (Train + Validation) to get max performance\n",
        "clf_full = lgb.LGBMClassifier(\n",
        "    n_estimators=2500,\n",
        "    learning_rate=0.02,\n",
        "    num_leaves=40,\n",
        "    random_state=42,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8\n",
        ")\n",
        "clf_full.fit(train_stacked[features], train_stacked['target_buy'])\n",
        "\n",
        "reg_full = lgb.LGBMRegressor(\n",
        "    objective='tweedie',\n",
        "    tweedie_variance_power=1.5,\n",
        "    n_estimators=2500,\n",
        "    learning_rate=0.02,\n",
        "    num_leaves=40,\n",
        "    random_state=42,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8\n",
        ")\n",
        "reg_full.fit(train_stacked[features], train_stacked['target_qty'])\n",
        "\n",
        "print(\"Generating Final Predictions...\")\n",
        "\n",
        "# Prepare Test sets for both horizons\n",
        "test_h1 = test_df_source.copy()\n",
        "test_h1['horizon'] = 1\n",
        "\n",
        "test_h2 = test_df_source.copy()\n",
        "test_h2['horizon'] = 2\n",
        "\n",
        "submission = test[['ID']].copy()\n",
        "\n",
        "# Horizon 1 Predictions\n",
        "submission['Target_purchase_next_1w'] = clf_full.predict_proba(test_h1[features])[:, 1]\n",
        "submission['Target_qty_next_1w'] = reg_full.predict(test_h1[features])\n",
        "\n",
        "# Horizon 2 Predictions\n",
        "submission['Target_purchase_next_2w'] = clf_full.predict_proba(test_h2[features])[:, 1]\n",
        "submission['Target_qty_next_2w'] = reg_full.predict(test_h2[features])\n",
        "\n",
        "# Final Cleanup (No negative quantities)\n",
        "submission['Target_qty_next_1w'] = submission['Target_qty_next_1w'].clip(lower=0)\n",
        "submission['Target_qty_next_2w'] = submission['Target_qty_next_2w'].clip(lower=0)\n",
        "\n",
        "submission.to_csv('submission_horizon_promax_scored.csv', index=False)\n",
        "print(\"Done! Saved as submission_horizon_promax_scored.csv\")"
      ],
      "metadata": {
        "id": "Kw93kEuMLCoJ"
      },
      "id": "Kw93kEuMLCoJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ADD THIS TO YOUR FEATURE ENGINEERING STEP ---\n",
        "\n",
        "# 1. Extract Date Features\n",
        "# These help the model learn seasonality (e.g., mango season)\n",
        "train['month'] = train['week_start'].dt.month\n",
        "train['week_of_year'] = train['week_start'].dt.isocalendar().week.astype(int)\n",
        "train['quarter'] = train['week_start'].dt.quarter\n",
        "\n",
        "test['month'] = test['week_start'].dt.month\n",
        "test['week_of_year'] = test['week_start'].dt.isocalendar().week.astype(int)\n",
        "test['quarter'] = test['week_start'].dt.quarter\n",
        "\n",
        "# 2. Calculate Customer Tenure (Days on Platform)\n",
        "# Older customers behave differently than new ones\n",
        "train['tenure_days'] = (train['week_start'] - train['customer_created_at']).dt.days\n",
        "test['tenure_days'] = (test['week_start'] - test['customer_created_at']).dt.days\n",
        "\n",
        "# 3. Add these new columns to your 'features' list\n",
        "# (Update the list in the Modeling Step)\n",
        "# features = ['month', 'week_of_year', 'tenure_days', 'customer_category', ...] + feature_cols"
      ],
      "metadata": {
        "id": "peksEIM2X-bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 845
        },
        "collapsed": true,
        "outputId": "9178bd6d-b41e-4b28-adc0-eaae5f1d1983"
      },
      "id": "peksEIM2X-bc",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "cannot convert NA to integer",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1709773297.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# These help the model learn seasonality (e.g., mango season)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'month'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'week_start'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'week_of_year'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'week_start'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misocalendar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweek\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'quarter'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'week_start'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquarter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   6641\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6642\u001b[0m             \u001b[0;31m# else, only a single dtype is given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6643\u001b[0;31m             \u001b[0mnew_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6644\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor_from_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6645\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"astype\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m         return self.apply(\n\u001b[0m\u001b[1;32m    431\u001b[0m             \u001b[0;34m\"astype\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m                 \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextend_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors, using_cow, squeeze)\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# type: ignore[call-overload]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mastype_array_safe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_coerce_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/dtypes/astype.py\u001b[0m in \u001b[0;36mastype_array_safe\u001b[0;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mastype_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;31m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/dtypes/astype.py\u001b[0m in \u001b[0;36mastype_array\u001b[0;34m(values, dtype, copy)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;31m# i.e. ExtensionArray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/arrays/masked.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;31m# to_numpy will also raise, but we get somewhat nicer exception messages here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m\"iu\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hasna\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cannot convert NA to integer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hasna\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;31m# careful: astype_nansafe converts np.nan to True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot convert NA to integer"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}