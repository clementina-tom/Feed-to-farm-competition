{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/clementina-tom/Feed-to-farm-competition/blob/main/pipeline_redo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a380d4f",
      "metadata": {
        "id": "2a380d4f"
      },
      "source": [
        "# Memory-Efficient ML Pipeline for Farm To Feed Dataset (Redo)\n",
        "\n",
        "This notebook implements a memory-efficient machine learning pipeline to predict customer purchasing behavior for 1-week and 2-week windows using pandas, gc, and LightGBM. Includes FileUpload widgets for easy file handling in Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ce9981c",
      "metadata": {
        "id": "5ce9981c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "import gc\n",
        "from sklearn.metrics import roc_auc_score, mean_absolute_error\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from ipywidgets import FileUpload\n",
        "import io"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73dfbfbd",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "5ba3ef028a5148c58fdc42feeacc12cb",
            "cd694b55523c418092b56573850f4124",
            "23329ffa4dca4b6ea944c88475fb29bf",
            "a94c88944b6543ba89f0a678c17cfc21"
          ]
        },
        "id": "73dfbfbd",
        "outputId": "2b40f00f-57a7-4d14-ad3b-583a296f95ca"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5ba3ef028a5148c58fdc42feeacc12cb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "FileUpload(value={}, accept='.csv', description='Train.csv')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cd694b55523c418092b56573850f4124",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "FileUpload(value={}, accept='.csv', description='Test.csv')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "23329ffa4dca4b6ea944c88475fb29bf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "FileUpload(value={}, accept='.csv', description='customer_data.csv')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a94c88944b6543ba89f0a678c17cfc21",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "FileUpload(value={}, accept='.csv', description='sku_data.csv')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# File Upload Widgets\n",
        "upload_train = FileUpload(accept='.csv', multiple=False, description='Train.csv')\n",
        "upload_test = FileUpload(accept='.csv', multiple=False, description='Test.csv')\n",
        "upload_customer = FileUpload(accept='.csv', multiple=False, description='customer_data.csv')\n",
        "upload_sku = FileUpload(accept='.csv', multiple=False, description='sku_data.csv')\n",
        "\n",
        "display(upload_train, upload_test, upload_customer, upload_sku)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "279ebab3",
      "metadata": {
        "id": "279ebab3",
        "outputId": "3458b472-1707-4b75-d696-381a9c15e2da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'upload_train' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-938236283.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Step 1: Efficient Data Loading & Downcasting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading data...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mupload_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupload_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'upload_train' is not defined"
          ]
        }
      ],
      "source": [
        "# Step 1: Efficient Data Loading & Downcasting\n",
        "print(\"Loading data...\")\n",
        "if upload_train.value:\n",
        "    train = pd.read_csv(io.BytesIO(list(upload_train.value.values())[0]['content']))\n",
        "else:\n",
        "    train = pd.read_csv('Train.csv')\n",
        "\n",
        "if upload_test.value:\n",
        "    test = pd.read_csv(io.BytesIO(list(upload_test.value.values())[0]['content']))\n",
        "else:\n",
        "    test = pd.read_csv('Test.csv')\n",
        "\n",
        "if upload_customer.value:\n",
        "    customer = pd.read_csv(io.BytesIO(list(upload_customer.value.values())[0]['content']))\n",
        "else:\n",
        "    customer = pd.read_csv('customer_data.csv')\n",
        "\n",
        "if upload_sku.value:\n",
        "    sku = pd.read_csv(io.BytesIO(list(upload_sku.value.values())[0]['content']))\n",
        "else:\n",
        "    sku = pd.read_csv('sku_data.csv')\n",
        "\n",
        "def downcast(df):\n",
        "    for col in df.select_dtypes(include=['float64']).columns:\n",
        "        df[col] = df[col].astype('float32')\n",
        "    for col in df.select_dtypes(include=['int64']).columns:\n",
        "        max_val = df[col].max()\n",
        "        if max_val < 2**8:\n",
        "            df[col] = df[col].astype('int8')\n",
        "        elif max_val < 2**16:\n",
        "            df[col] = df[col].astype('int16')\n",
        "        else:\n",
        "            df[col] = df[col].astype('int32')\n",
        "    return df\n",
        "\n",
        "train = downcast(train)\n",
        "test = downcast(test)\n",
        "customer = downcast(customer)\n",
        "sku = downcast(sku)\n",
        "\n",
        "train['week_start'] = pd.to_datetime(train['week_start'])\n",
        "test['week_start'] = pd.to_datetime(test['week_start'])\n",
        "customer['customer_created_at'] = pd.to_datetime(customer['customer_created_at'])\n",
        "\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fa8fe05",
      "metadata": {
        "id": "4fa8fe05"
      },
      "outputs": [],
      "source": [
        "# Merge additional data\n",
        "train = train.merge(customer, on='customer_id', how='left', suffixes=('', '_cust'))\n",
        "train = train.merge(sku, on='product_unit_variant_id', how='left', suffixes=('', '_sku'))\n",
        "test = test.merge(customer, on='customer_id', how='left', suffixes=('', '_cust'))\n",
        "test = test.merge(sku, on='product_unit_variant_id', how='left', suffixes=('', '_sku'))\n",
        "\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "886ccfd0",
      "metadata": {
        "id": "886ccfd0"
      },
      "outputs": [],
      "source": [
        "# Step 2: Smart Grid Creation & Feature Engineering\n",
        "full_df = pd.concat([train[['customer_id', 'product_unit_variant_id', 'week_start', 'qty_this_week']],\n",
        "                     test[['customer_id', 'product_unit_variant_id', 'week_start']].assign(qty_this_week=0.0)])\n",
        "full_df = downcast(full_df)\n",
        "full_df = full_df.sort_values(['customer_id', 'product_unit_variant_id', 'week_start'])\n",
        "\n",
        "print(\"Creating features...\")\n",
        "full_df['lag1_qty'] = full_df.groupby(['customer_id', 'product_unit_variant_id'])['qty_this_week'].shift(1)\n",
        "full_df['lag2_qty'] = full_df.groupby(['customer_id', 'product_unit_variant_id'])['qty_this_week'].shift(2)\n",
        "full_df['lag3_qty'] = full_df.groupby(['customer_id', 'product_unit_variant_id'])['qty_this_week'].shift(3)\n",
        "full_df['rolling_mean_4'] = full_df.groupby(['customer_id', 'product_unit_variant_id'])['qty_this_week'].rolling(4).mean().reset_index(0, drop=True)\n",
        "full_df['rolling_max_4'] = full_df.groupby(['customer_id', 'product_unit_variant_id'])['qty_this_week'].rolling(4).max().reset_index(0, drop=True)\n",
        "\n",
        "full_df[['lag1_qty', 'lag2_qty', 'lag3_qty', 'rolling_mean_4', 'rolling_max_4']] = full_df[['lag1_qty', 'lag2_qty', 'lag3_qty', 'rolling_mean_4', 'rolling_max_4']].fillna(0)\n",
        "\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcd90edd",
      "metadata": {
        "id": "dcd90edd"
      },
      "outputs": [],
      "source": [
        "# Merge features back\n",
        "feature_cols = ['lag1_qty', 'lag2_qty', 'lag3_qty', 'rolling_mean_4', 'rolling_max_4']\n",
        "train = train.merge(full_df[['customer_id', 'product_unit_variant_id', 'week_start'] + feature_cols],\n",
        "                    on=['customer_id', 'product_unit_variant_id', 'week_start'], how='left')\n",
        "test = test.merge(full_df[['customer_id', 'product_unit_variant_id', 'week_start'] + feature_cols],\n",
        "                  on=['customer_id', 'product_unit_variant_id', 'week_start'], how='left')\n",
        "\n",
        "del full_df\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69fb463f",
      "metadata": {
        "id": "69fb463f"
      },
      "outputs": [],
      "source": [
        "# Step 3: Target Generation\n",
        "print(\"Creating targets...\")\n",
        "train = train.sort_values(['customer_id', 'product_unit_variant_id', 'week_start'])\n",
        "train['target_purchase_1w'] = (train.groupby(['customer_id', 'product_unit_variant_id'])['qty_this_week'].shift(-1) > 0).astype(int)\n",
        "train['target_qty_1w'] = train.groupby(['customer_id', 'product_unit_variant_id'])['qty_this_week'].shift(-1).fillna(0)\n",
        "train['target_purchase_2w'] = (train.groupby(['customer_id', 'product_unit_variant_id'])['qty_this_week'].shift(-2) > 0).astype(int)\n",
        "train['target_qty_2w'] = train.groupby(['customer_id', 'product_unit_variant_id'])['qty_this_week'].shift(-2).fillna(0)\n",
        "\n",
        "train = train.dropna(subset=['target_purchase_1w'])\n",
        "\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68ec0dbf",
      "metadata": {
        "id": "68ec0dbf"
      },
      "outputs": [],
      "source": [
        "# Encode categoricals\n",
        "cat_cols = ['customer_category', 'customer_status', 'grade_name', 'unit_name']\n",
        "le = LabelEncoder()\n",
        "for col in cat_cols:\n",
        "    combined = pd.concat([train[col], test[col]])\n",
        "    le.fit(combined)\n",
        "    train[col] = le.transform(train[col])\n",
        "    test[col] = le.transform(test[col])\n",
        "\n",
        "features = ['selling_price', 'customer_category', 'customer_status', 'grade_name', 'unit_name'] + feature_cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42f40c55",
      "metadata": {
        "id": "42f40c55"
      },
      "outputs": [],
      "source": [
        "# Step 4: Modeling\n",
        "print(\"Training models...\")\n",
        "weeks = sorted(train['week_start'].unique())\n",
        "val_weeks = weeks[-2:]\n",
        "val_mask = train['week_start'].isin(val_weeks)\n",
        "\n",
        "models = {}\n",
        "targets = ['target_purchase_1w', 'target_qty_1w', 'target_purchase_2w', 'target_qty_2w']\n",
        "for target in targets:\n",
        "    print(f\"Training {target}...\")\n",
        "    X = train[features]\n",
        "    y = train[target]\n",
        "    X_train, X_val = X[~val_mask], X[val_mask]\n",
        "    y_train, y_val = y[~val_mask], y[val_mask]\n",
        "\n",
        "    if 'purchase' in target:\n",
        "        model = lgb.LGBMClassifier(n_estimators=1000, early_stopping_rounds=50, verbose=-1)\n",
        "        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], eval_metric='auc')\n",
        "        pred_val = model.predict_proba(X_val)[:, 1]\n",
        "        auc = roc_auc_score(y_val, pred_val)\n",
        "        print(f\"AUC for {target}: {auc}\")\n",
        "    else:\n",
        "        model = lgb.LGBMRegressor(n_estimators=1000, early_stopping_rounds=50, verbose=-1)\n",
        "        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], eval_metric='mae')\n",
        "        pred_val = model.predict(X_val)\n",
        "        mae = mean_absolute_error(y_val, pred_val)\n",
        "        print(f\"MAE for {target}: {mae}\")\n",
        "\n",
        "    models[target] = model\n",
        "    gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95061155",
      "metadata": {
        "id": "95061155"
      },
      "outputs": [],
      "source": [
        "# Step 5: Submission\n",
        "print(\"Generating predictions...\")\n",
        "test['Target_purchase_next_1w'] = models['target_purchase_1w'].predict_proba(test[features])[:, 1]\n",
        "test['Target_qty_next_1w'] = models['target_qty_1w'].predict(test[features])\n",
        "test['Target_purchase_next_2w'] = models['target_purchase_2w'].predict_proba(test[features])[:, 1]\n",
        "test['Target_qty_next_2w'] = models['target_qty_2w'].predict(test[features])\n",
        "\n",
        "submission = test[['ID', 'Target_purchase_next_1w', 'Target_qty_next_1w', 'Target_purchase_next_2w', 'Target_qty_next_2w']]\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "print(\"Submission saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JlFp5PP0IidY"
      },
      "id": "JlFp5PP0IidY"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "import gc\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# --- 1. Load Data ---\n",
        "print(\"Loading...\")\n",
        "train = pd.read_csv('Train.csv')\n",
        "test = pd.read_csv('Test.csv')\n",
        "customer = pd.read_csv('customer_data.csv') # Load customer data\n",
        "sku = pd.read_csv('sku_data.csv')\n",
        "\n",
        "# --- 2. Downcast & Date Conversion ---\n",
        "def downcast(df):\n",
        "    for col in df.select_dtypes(include=['float64']).columns:\n",
        "        df[col] = df[col].astype('float32')\n",
        "    for col in df.select_dtypes(include=['int64']).columns:\n",
        "        # Adjusted int downcasting to avoid overflow for larger values\n",
        "        max_val = df[col].max()\n",
        "        min_val = df[col].min()\n",
        "        if min_val >= np.iinfo('int8').min and max_val <= np.iinfo('int8').max:\n",
        "            df[col] = df[col].astype('int8')\n",
        "        elif min_val >= np.iinfo('int16').min and max_val <= np.iinfo('int16').max:\n",
        "            df[col] = df[col].astype('int16')\n",
        "        elif min_val >= np.iinfo('int32').min and max_val <= np.iinfo('int32').max:\n",
        "            df[col] = df[col].astype('int32')\n",
        "    return df\n",
        "\n",
        "train = downcast(train)\n",
        "test = downcast(test)\n",
        "customer = downcast(customer) # Downcast customer data\n",
        "sku = downcast(sku)\n",
        "\n",
        "train['week_start'] = pd.to_datetime(train['week_start'])\n",
        "test['week_start'] = pd.to_datetime(test['week_start'])\n",
        "customer['customer_created_at'] = pd.to_datetime(customer['customer_created_at'])\n",
        "\n",
        "# --- Merge additional data (customer and sku) into train and test FIRST ---\n",
        "# This ensures all categorical and customer_created_at columns are present in train/test\n",
        "train = train.merge(customer, on='customer_id', how='left')\n",
        "train = train.merge(sku, on='product_unit_variant_id', how='left')\n",
        "test = test.merge(customer, on='customer_id', how='left')\n",
        "test = test.merge(sku, on='product_unit_variant_id', how='left')\n",
        "\n",
        "print(\"Train columns after merges:\", train.columns.tolist())\n",
        "print(\"Test columns after merges:\", test.columns.tolist())\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "# --- 3. Feature Engineering (Unified) ---\n",
        "# Combine Train and Test to generate Lag Features smoothly\n",
        "# Drop 'ID' column before concat as it's not a feature and could be duplicated.\n",
        "test_with_qty = test.assign(qty_this_week=0.0).drop(columns=['ID'], errors='ignore')\n",
        "full_df = pd.concat([train.drop(columns=['ID'], errors='ignore'), test_with_qty], ignore_index=True)\n",
        "\n",
        "# The merges with customer and sku are now done earlier on train/test, no need to merge into full_df again here.\n",
        "\n",
        "full_df = downcast(full_df) # Downcast full_df after concat\n",
        "full_df = full_df.sort_values(['customer_id', 'product_unit_variant_id', 'week_start'])\n",
        "\n",
        "print(\"Full_df columns after concat and sort:\", full_df.columns.tolist())\n",
        "\n",
        "# Generate Lags\n",
        "print(\"Generating features...\")\n",
        "grp = full_df.groupby(['customer_id', 'product_unit_variant_id'])['qty_this_week']\n",
        "full_df['lag1'] = grp.shift(1)\n",
        "full_df['lag2'] = grp.shift(2)\n",
        "full_df['roll_mean_4'] = grp.rolling(4).mean().reset_index(level=[0,1], drop=True)\n",
        "\n",
        "# Fill NaNs\n",
        "full_df[['lag1', 'lag2', 'roll_mean_4']] = full_df[['lag1', 'lag2', 'roll_mean_4']].fillna(0)\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "# --- 4. Create \"Stacked\" Training Data (The Grandmaster Move) ---\n",
        "# We transform the data:\n",
        "# Row 1: Target=Week1, Horizon=1\n",
        "# Row 2: Target=Week2, Horizon=2\n",
        "\n",
        "# Ensure all relevant columns are carried over from full_df\n",
        "# Use all columns that will be features or used for stacking\n",
        "relevant_cols = ['customer_id', 'product_unit_variant_id', 'week_start', 'qty_this_week',\n",
        "                 'customer_category_x', 'customer_status_x', 'grade_name_x', 'unit_name_x',\n",
        "                 'lag1', 'lag2', 'roll_mean_4', 'customer_created_at_x'] # Use _x suffixes for these columns\n",
        "\n",
        "print(\"Full_df columns before train_df/test_df split:\", full_df.columns.tolist())\n",
        "\n",
        "train_df = full_df[full_df['week_start'].isin(train['week_start'])][relevant_cols].copy()\n",
        "test_df = full_df[full_df['week_start'].isin(test['week_start'])][relevant_cols].copy()\n",
        "\n",
        "# Create Horizon 1 Subset\n",
        "h1 = train_df.copy()\n",
        "h1['target_qty'] = h1.groupby(['customer_id', 'product_unit_variant_id'])['qty_this_week'].shift(-1)\n",
        "h1['target_buy'] = (h1['target_qty'] > 0).astype(int)\n",
        "h1['horizon'] = 1\n",
        "\n",
        "# Create Horizon 2 Subset\n",
        "h2 = train_df.copy()\n",
        "h2['target_qty'] = h2.groupby(['customer_id', 'product_unit_variant_id'])['qty_this_week'].shift(-2)\n",
        "h2['target_buy'] = (h2['target_qty'] > 0).astype(int)\n",
        "h2['horizon'] = 2\n",
        "\n",
        "# Stack them!\n",
        "train_stacked = pd.concat([h1, h2], ignore_index=True)\n",
        "train_stacked = train_stacked.dropna(subset=['target_qty']) # Remove rows where target is NaN (end of data)\n",
        "\n",
        "# --- 5. Train Unified Models ---\n",
        "# Add the new merged features to the list of features, including customer_category and customer_status\n",
        "features = ['lag1', 'lag2', 'roll_mean_4', 'horizon',\n",
        "            'customer_category_x', 'customer_status_x', 'grade_name_x', 'unit_name_x'] # Use _x suffixes\n",
        "\n",
        "# Before training, we need to LabelEncode the categorical features\n",
        "cat_cols_to_encode = ['customer_category_x', 'customer_status_x', 'grade_name_x', 'unit_name_x'] # Use _x suffixes\n",
        "le = LabelEncoder()\n",
        "for col in cat_cols_to_encode:\n",
        "    # Fit on combined data from train_stacked, test_h1, test_h2 for consistent encoding\n",
        "    # Note: test_df is used for encoding consistency across train_stacked and final test predictions\n",
        "    combined_data = pd.concat([train_stacked[col], test_df[col]]).astype(str).fillna('UNKNOWN')\n",
        "    le.fit(combined_data)\n",
        "\n",
        "    train_stacked[col] = le.transform(train_stacked[col].astype(str).fillna('UNKNOWN'))\n",
        "    test_df[col] = le.transform(test_df[col].astype(str).fillna('UNKNOWN'))\n",
        "\n",
        "# Prepare Test Set for H1 and H2 BEFORE encoding customer data to ensure test_df is used correctly\n",
        "test_h1 = test_df.copy()\n",
        "test_h2 = test_df.copy()\n",
        "\n",
        "# Apply encoding to test_h1 and test_h2 based on the encoded test_df (already done above, but confirm this line is still needed for clarity if test_h1/h2 were not direct copies)\n",
        "# This line ensures test_h1 and test_h2 get the encoded values from test_df\n",
        "test_h1[cat_cols_to_encode] = test_df[cat_cols_to_encode]\n",
        "test_h2[cat_cols_to_encode] = test_df[cat_cols_to_encode]\n",
        "\n",
        "print(\"Training Unified Classifier...\")\n",
        "clf = lgb.LGBMClassifier(n_estimators=1500, learning_rate=0.05)\n",
        "clf.fit(train_stacked[features], train_stacked['target_buy'])\n",
        "\n",
        "print(\"Training Unified Regressor (Tweedie)...\")\n",
        "reg = lgb.LGBMRegressor(objective='tweedie', n_estimators=1500, learning_rate=0.05)\n",
        "reg.fit(train_stacked[features], train_stacked['target_qty'])\n",
        "\n",
        "# --- 6. Predict ---\n",
        "test_h1['horizon'] = 1\n",
        "test_h2['horizon'] = 2\n",
        "\n",
        "submission = test[['ID']].copy()\n",
        "submission['Target_purchase_next_1w'] = clf.predict_proba(test_h1[features])[:, 1]\n",
        "submission['Target_qty_next_1w'] = reg.predict(test_h1[features])\n",
        "submission['Target_purchase_next_2w'] = clf.predict_proba(test_h2[features])[:, 1]\n",
        "submission['Target_qty_next_2w'] = reg.predict(test_h2[features])\n",
        "\n",
        "submission.to_csv('submission_horizon.csv', index=False)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CMyzMUIIeld",
        "outputId": "9bdf3e08-b70a-432d-dce9-4bf903ea3de2"
      },
      "id": "6CMyzMUIIeld",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading...\n",
            "Train columns after merges: ['ID', 'customer_id', 'product_unit_variant_id', 'week_start', 'qty_this_week', 'num_orders_week', 'spend_this_week', 'purchased_this_week', 'product_id', 'grade_name_x', 'unit_name_x', 'product_grade_variant_id', 'selling_price', 'customer_category_x', 'customer_status_x', 'customer_created_at_x', 'Target_qty_next_1w', 'Target_purchase_next_1w', 'Target_qty_next_2w', 'Target_purchase_next_2w', 'customer_category_y', 'customer_status_y', 'customer_created_at_y', 'product_name', 'product_grade_variant_sku', 'unit_name_y', 'grade_name_y', 'grade_active_status']\n",
            "Test columns after merges: ['ID', 'customer_id', 'product_unit_variant_id', 'week_start', 'product_id', 'grade_name_x', 'unit_name_x', 'product_grade_variant_id', 'customer_category_x', 'customer_status_x', 'customer_created_at_x', 'customer_category_y', 'customer_status_y', 'customer_created_at_y', 'product_name', 'product_grade_variant_sku', 'unit_name_y', 'grade_name_y', 'grade_active_status']\n",
            "Full_df columns after concat and sort: ['customer_id', 'product_unit_variant_id', 'week_start', 'qty_this_week', 'num_orders_week', 'spend_this_week', 'purchased_this_week', 'product_id', 'grade_name_x', 'unit_name_x', 'product_grade_variant_id', 'selling_price', 'customer_category_x', 'customer_status_x', 'customer_created_at_x', 'Target_qty_next_1w', 'Target_purchase_next_1w', 'Target_qty_next_2w', 'Target_purchase_next_2w', 'customer_category_y', 'customer_status_y', 'customer_created_at_y', 'product_name', 'product_grade_variant_sku', 'unit_name_y', 'grade_name_y', 'grade_active_status']\n",
            "Generating features...\n",
            "Full_df columns before train_df/test_df split: ['customer_id', 'product_unit_variant_id', 'week_start', 'qty_this_week', 'num_orders_week', 'spend_this_week', 'purchased_this_week', 'product_id', 'grade_name_x', 'unit_name_x', 'product_grade_variant_id', 'selling_price', 'customer_category_x', 'customer_status_x', 'customer_created_at_x', 'Target_qty_next_1w', 'Target_purchase_next_1w', 'Target_qty_next_2w', 'Target_purchase_next_2w', 'customer_category_y', 'customer_status_y', 'customer_created_at_y', 'product_name', 'product_grade_variant_sku', 'unit_name_y', 'grade_name_y', 'grade_active_status', 'lag1', 'lag2', 'roll_mean_4']\n",
            "Training Unified Classifier...\n",
            "Training Unified Regressor (Tweedie)...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "import gc\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# --- 1. Load Data ---\n",
        "print(\"Loading data...\")\n",
        "train = pd.read_csv('Train.csv')\n",
        "test = pd.read_csv('Test.csv')\n",
        "customer = pd.read_csv('customer_data.csv')\n",
        "sku = pd.read_csv('sku_data.csv')\n",
        "\n",
        "# --- 2. Downcast & Pre-processing ---\n",
        "def downcast(df):\n",
        "    for col in df.select_dtypes(include=['float64']).columns:\n",
        "        df[col] = df[col].astype('float32')\n",
        "    for col in df.select_dtypes(include=['int64']).columns:\n",
        "        if df[col].max() < 2**32:\n",
        "            df[col] = df[col].astype('int32')\n",
        "    return df\n",
        "\n",
        "train = downcast(train)\n",
        "test = downcast(test)\n",
        "customer = downcast(customer)\n",
        "sku = downcast(sku)\n",
        "\n",
        "# Convert Dates\n",
        "train['week_start'] = pd.to_datetime(train['week_start'])\n",
        "test['week_start'] = pd.to_datetime(test['week_start'])\n",
        "customer['customer_created_at'] = pd.to_datetime(customer['customer_created_at'])\n",
        "\n",
        "# --- 3. Unified Feature Engineering (The \"Full Grid\" Approach) ---\n",
        "print(\"Building Full History...\")\n",
        "\n",
        "# A. Combine Train and Test FIRST\n",
        "# We need the Test set to exist so that 'shift(-1)' on the last week of Train\n",
        "# can actually see the first week of Test (if applicable) or handle boundaries correctly.\n",
        "full_df = pd.concat([\n",
        "    train[['customer_id', 'product_unit_variant_id', 'week_start', 'qty_this_week']],\n",
        "    test[['customer_id', 'product_unit_variant_id', 'week_start']].assign(qty_this_week=0.0)\n",
        "], ignore_index=True)\n",
        "\n",
        "full_df = full_df.sort_values(['customer_id', 'product_unit_variant_id', 'week_start'])\n",
        "\n",
        "# B. Add \"Grandmaster Features\" (Seasonality & Trends)\n",
        "# Fix: Handle potential NaT values before converting to int\n",
        "full_df['month'] = full_df['week_start'].dt.month.fillna(0).astype(int)\n",
        "full_df['week_of_year'] = full_df['week_start'].dt.isocalendar().week.fillna(0).astype(int)\n",
        "\n",
        "# C. Global Product Trend (Shifted to prevent leakage)\n",
        "# \"How much is this product selling globally across all customers?\"\n",
        "prod_trend = full_df.groupby(['product_unit_variant_id', 'week_start'])['qty_this_week'].mean().reset_index().rename(columns={'qty_this_week': 'global_qty'})\n",
        "prod_trend = prod_trend.sort_values(['product_unit_variant_id', 'week_start'])\n",
        "prod_trend['global_trend_lag1'] = prod_trend.groupby('product_unit_variant_id')['global_qty'].shift(1)\n",
        "full_df = full_df.merge(prod_trend[['product_unit_variant_id', 'week_start', 'global_trend_lag1']],\n",
        "                        on=['product_unit_variant_id', 'week_start'], how='left')\n",
        "\n",
        "# D. Lag Features\n",
        "print(\"Generating lags...\")\n",
        "grp = full_df.groupby(['customer_id', 'product_unit_variant_id'])['qty_this_week']\n",
        "full_df['lag1'] = grp.shift(1)\n",
        "full_df['lag2'] = grp.shift(2)\n",
        "full_df['roll_mean_4'] = grp.rolling(4).mean().reset_index(level=[0,1], drop=True)\n",
        "\n",
        "# --- 4. Target Generation (CRITICAL FIX) ---\n",
        "# We generate targets ON THE FULL DATASET so the shifts work correctly.\n",
        "print(\"Generating Targets on Full Data...\")\n",
        "full_df['target_qty_1w'] = grp.shift(-1) # Next week's qty\n",
        "full_df['target_qty_2w'] = grp.shift(-2) # Week after next qty\n",
        "\n",
        "# --- 5. Merge Metadata ---\n",
        "# Now we bring in Customer and SKU details\n",
        "full_df = full_df.merge(customer, on='customer_id', how='left')\n",
        "full_df = full_df.merge(sku, on='product_unit_variant_id', how='left')\n",
        "\n",
        "# Encode Categoricals\n",
        "cat_cols = ['customer_category', 'customer_status', 'grade_name', 'unit_name']\n",
        "le = LabelEncoder()\n",
        "for col in cat_cols:\n",
        "    full_df[col] = full_df[col].astype(str).fillna('UNKNOWN')\n",
        "    full_df[col] = le.fit_transform(full_df[col])\n",
        "\n",
        "# Fill Numerical NaNs\n",
        "num_cols = ['lag1', 'lag2', 'roll_mean_4', 'global_trend_lag1']\n",
        "full_df[num_cols] = full_df[num_cols].fillna(0)\n",
        "\n",
        "# --- 6. Create Stacked Training Data ---\n",
        "print(\"Stacking Data...\")\n",
        "\n",
        "# Identify Train rows vs Test rows\n",
        "# Train rows are those that were in the original Train CSV\n",
        "train_mask = full_df['week_start'].isin(train['week_start'].unique())\n",
        "train_df_source = full_df[train_mask].copy()\n",
        "test_df_source = full_df[~train_mask].copy()\n",
        "\n",
        "# Stack 1: Horizon 1\n",
        "h1 = train_df_source.copy()\n",
        "h1['target_qty'] = h1['target_qty_1w']\n",
        "h1['target_buy'] = (h1['target_qty'] > 0).astype(int)\n",
        "h1['horizon'] = 1\n",
        "\n",
        "# Stack 2: Horizon 2\n",
        "h2 = train_df_source.copy()\n",
        "h2['target_qty'] = h2['target_qty_2w']\n",
        "h2['target_buy'] = (h2['target_qty'] > 0).astype(int)\n",
        "h2['horizon'] = 2\n",
        "\n",
        "# Combine\n",
        "train_stacked = pd.concat([h1, h2], ignore_index=True)\n",
        "\n",
        "# Drop rows where target is NaN (End of history)\n",
        "train_stacked = train_stacked.dropna(subset=['target_qty'])\n",
        "\n",
        "# Define Features\n",
        "features = ['lag1', 'lag2', 'roll_mean_4', 'global_trend_lag1',\n",
        "            'month', 'week_of_year', 'horizon',\n",
        "            'customer_category', 'customer_status', 'grade_name', 'unit_name']\n",
        "\n",
        "# --- 7. Modeling ---\n",
        "print(\"Training Unified Models...\")\n",
        "\n",
        "# Classifier (AUC)\n",
        "clf = lgb.LGBMClassifier(n_estimators=2000, learning_rate=0.03, num_leaves=31, random_state=42)\n",
        "clf.fit(train_stacked[features], train_stacked['target_buy'])\n",
        "\n",
        "# Regressor (Tweedie for MAE)\n",
        "reg = lgb.LGBMRegressor(objective='tweedie', tweedie_variance_power=1.5,\n",
        "                        n_estimators=2000, learning_rate=0.03, num_leaves=31, random_state=42)\n",
        "reg.fit(train_stacked[features], train_stacked['target_qty'])\n",
        "\n",
        "# --- 8. Prediction ---\n",
        "print(\"Generating Predictions...\")\n",
        "\n",
        "# Prepare Test sets\n",
        "test_h1 = test_df_source.copy()\n",
        "test_h1['horizon'] = 1\n",
        "\n",
        "test_h2 = test_df_source.copy()\n",
        "test_h2['horizon'] = 2\n",
        "\n",
        "# Predict\n",
        "submission = test[['ID']].copy()\n",
        "submission['Target_purchase_next_1w'] = clf.predict_proba(test_h1[features])[:, 1]\n",
        "submission['Target_qty_next_1w'] = reg.predict(test_h1[features])\n",
        "submission['Target_purchase_next_2w'] = clf.predict_proba(test_h2[features])[:, 1]\n",
        "submission['Target_qty_next_2w'] = reg.predict(test_h2[features])\n",
        "\n",
        "# --- 9. Consistency Check ---\n",
        "# If probability of buy is very low, force quantity to 0?\n",
        "# (Optional, but helps MAE). Let's be conservative.\n",
        "# submission.loc[submission['Target_purchase_next_1w'] < 0.1, 'Target_qty_next_1w'] = 0\n",
        "# submission.loc[submission['Target_purchase_next_2w'] < 0.1, 'Target_qty_next_2w'] = 0\n",
        "\n",
        "submission.to_csv('submission_horizon_fixed.csv', index=False)\n",
        "print(\"Done! Saved as submission_horizon_fixed.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTxCsTJ4pfoX",
        "outputId": "a8aa24bc-81e2-4b31-bf50-69e87045cd71"
      },
      "id": "MTxCsTJ4pfoX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Building Full History...\n",
            "Generating lags...\n",
            "Generating Targets on Full Data...\n",
            "Stacking Data...\n",
            "Training Unified Models...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "import gc\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# --- 1. Load Data ---\n",
        "print(\"Loading data...\")\n",
        "train = pd.read_csv('Train.csv')\n",
        "test = pd.read_csv('Test.csv')\n",
        "customer = pd.read_csv('customer_data.csv')\n",
        "sku = pd.read_csv('sku_data.csv')\n",
        "\n",
        "# --- 2. Downcast to Save Memory ---\n",
        "def downcast(df):\n",
        "    for col in df.select_dtypes(include=['float64']).columns:\n",
        "        df[col] = df[col].astype('float32')\n",
        "    for col in df.select_dtypes(include=['int64']).columns:\n",
        "        if df[col].max() < 2**32:\n",
        "            df[col] = df[col].astype('int32')\n",
        "    return df\n",
        "\n",
        "train = downcast(train)\n",
        "test = downcast(test)\n",
        "customer = downcast(customer)\n",
        "sku = downcast(sku)\n",
        "\n",
        "# Convert Dates\n",
        "train['week_start'] = pd.to_datetime(train['week_start'])\n",
        "test['week_start'] = pd.to_datetime(test['week_start'])\n",
        "customer['customer_created_at'] = pd.to_datetime(customer['customer_created_at'])\n",
        "\n",
        "# --- 3. Unified Feature Engineering ---\n",
        "print(\"Building Full History...\")\n",
        "\n",
        "# A. Combine Train and Test FIRST\n",
        "# We need the full timeline to calculate trends and lags correctly\n",
        "full_df = pd.concat([\n",
        "    train[['customer_id', 'product_unit_variant_id', 'week_start', 'qty_this_week']],\n",
        "    test[['customer_id', 'product_unit_variant_id', 'week_start']].assign(qty_this_week=0.0)\n",
        "], ignore_index=True)\n",
        "\n",
        "full_df = full_df.sort_values(['customer_id', 'product_unit_variant_id', 'week_start'])\n",
        "\n",
        "# --- GRANDMASTER FEATURES ---\n",
        "\n",
        "# 1. Seasonality (Time)\n",
        "full_df['month'] = full_df['week_start'].dt.month.fillna(0).astype(int)\n",
        "full_df['week_of_year'] = full_df['week_start'].dt.isocalendar().week.fillna(0).astype(int)\n",
        "\n",
        "# 2. Global Product Trend (The \"Viral\" Factor)\n",
        "# Calculates: \"How much is this specific product selling across ALL customers right now?\"\n",
        "# We shift by 1 week so we don't peek into the future.\n",
        "prod_trend = full_df.groupby(['product_unit_variant_id', 'week_start'])['qty_this_week'].mean().reset_index().rename(columns={'qty_this_week': 'global_qty'})\n",
        "prod_trend = prod_trend.sort_values(['product_unit_variant_id', 'week_start'])\n",
        "prod_trend['global_trend_lag1'] = prod_trend.groupby('product_unit_variant_id')['global_qty'].shift(1)\n",
        "prod_trend['global_trend_roll4'] = prod_trend.groupby('product_unit_variant_id')['global_qty'].transform(lambda x: x.shift(1).rolling(4).mean())\n",
        "\n",
        "full_df = full_df.merge(prod_trend[['product_unit_variant_id', 'week_start', 'global_trend_lag1', 'global_trend_roll4']],\n",
        "                        on=['product_unit_variant_id', 'week_start'], how='left')\n",
        "\n",
        "# 3. Lag Features (Individual History)\n",
        "print(\"Generating lags...\")\n",
        "grp = full_df.groupby(['customer_id', 'product_unit_variant_id'])['qty_this_week']\n",
        "full_df['lag1'] = grp.shift(1)\n",
        "full_df['lag2'] = grp.shift(2)\n",
        "full_df['lag3'] = grp.shift(3)\n",
        "full_df['roll_mean_4'] = grp.rolling(4).mean().reset_index(level=[0,1], drop=True)\n",
        "full_df['roll_max_4'] = grp.rolling(4).max().reset_index(level=[0,1], drop=True)\n",
        "\n",
        "# --- 4. Target Generation (THE FIX) ---\n",
        "# Generate targets on the full timeline BEFORE splitting\n",
        "print(\"Generating Targets...\")\n",
        "full_df['target_qty_1w'] = grp.shift(-1) # Next week's qty\n",
        "full_df['target_qty_2w'] = grp.shift(-2) # Week after next qty\n",
        "\n",
        "# --- 5. Merge Metadata & Encode ---\n",
        "full_df = full_df.merge(customer, on='customer_id', how='left')\n",
        "full_df = full_df.merge(sku, on='product_unit_variant_id', how='left')\n",
        "\n",
        "# Customer Tenure (How long have they been with us?)\n",
        "full_df['customer_tenure_days'] = (full_df['week_start'] - full_df['customer_created_at']).dt.days\n",
        "\n",
        "# Encode Categoricals\n",
        "cat_cols = ['customer_category', 'customer_status', 'grade_name', 'unit_name']\n",
        "le = LabelEncoder()\n",
        "for col in cat_cols:\n",
        "    full_df[col] = full_df[col].astype(str).fillna('UNKNOWN')\n",
        "    full_df[col] = le.fit_transform(full_df[col])\n",
        "\n",
        "# Fill NaNs in features with 0\n",
        "num_cols = ['lag1', 'lag2', 'lag3', 'roll_mean_4', 'roll_max_4', 'global_trend_lag1', 'global_trend_roll4', 'customer_tenure_days']\n",
        "full_df[num_cols] = full_df[num_cols].fillna(0)\n",
        "\n",
        "# --- 6. Create Stacked Training Data ---\n",
        "print(\"Stacking Data...\")\n",
        "\n",
        "# Filter back to Train and Test sets\n",
        "train_mask = full_df['week_start'].isin(train['week_start'].unique())\n",
        "train_df_source = full_df[train_mask].copy()\n",
        "test_df_source = full_df[~train_mask].copy()\n",
        "\n",
        "# Stack 1: Horizon 1 (Predicting Next Week)\n",
        "h1 = train_df_source.copy()\n",
        "h1['target_qty'] = h1['target_qty_1w']\n",
        "h1['target_buy'] = (h1['target_qty'] > 0).astype(int)\n",
        "h1['horizon'] = 1\n",
        "\n",
        "# Stack 2: Horizon 2 (Predicting 2 Weeks out)\n",
        "h2 = train_df_source.copy()\n",
        "h2['target_qty'] = h2['target_qty_2w']\n",
        "h2['target_buy'] = (h2['target_qty'] > 0).astype(int)\n",
        "h2['horizon'] = 2\n",
        "\n",
        "# Combine Stacks\n",
        "train_stacked = pd.concat([h1, h2], ignore_index=True)\n",
        "train_stacked = train_stacked.dropna(subset=['target_qty']) # Valid targets only\n",
        "\n",
        "# Define Features\n",
        "features = [\n",
        "    'lag1', 'lag2', 'lag3', 'roll_mean_4', 'roll_max_4', # Individual behavior\n",
        "    'global_trend_lag1', 'global_trend_roll4',           # Market behavior\n",
        "    'month', 'week_of_year', 'horizon',                  # Time context\n",
        "    'customer_tenure_days',                              # Loyalty\n",
        "    'customer_category', 'customer_status', 'grade_name', 'unit_name' # Metadata\n",
        "]\n",
        "\n",
        "# --- 7. Modeling (High Precision) ---\n",
        "print(\"Training Unified Models...\")\n",
        "\n",
        "# Classifier (Optimized for AUC)\n",
        "# Lower learning rate (0.02) + More trees (2500) = Better Pattern Recognition\n",
        "clf = lgb.LGBMClassifier(\n",
        "    n_estimators=2500,\n",
        "    learning_rate=0.02,\n",
        "    num_leaves=40,\n",
        "    random_state=42,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8\n",
        ")\n",
        "clf.fit(train_stacked[features], train_stacked['target_buy'])\n",
        "print(\"Classifier Trained.\")\n",
        "\n",
        "# Regressor (Optimized for Tweedie/MAE)\n",
        "reg = lgb.LGBMRegressor(\n",
        "    objective='tweedie',\n",
        "    tweedie_variance_power=1.5,\n",
        "    n_estimators=2500,\n",
        "    learning_rate=0.02,\n",
        "    num_leaves=40,\n",
        "    random_state=42,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8\n",
        ")\n",
        "reg.fit(train_stacked[features], train_stacked['target_qty'])\n",
        "print(\"Regressor Trained.\")\n",
        "\n",
        "# --- 8. Prediction ---\n",
        "print(\"Generating Predictions...\")\n",
        "\n",
        "# Prepare Test sets for both horizons\n",
        "test_h1 = test_df_source.copy()\n",
        "test_h1['horizon'] = 1\n",
        "\n",
        "test_h2 = test_df_source.copy()\n",
        "test_h2['horizon'] = 2\n",
        "\n",
        "submission = test[['ID']].copy()\n",
        "\n",
        "# Horizon 1 Predictions\n",
        "submission['Target_purchase_next_1w'] = clf.predict_proba(test_h1[features])[:, 1]\n",
        "submission['Target_qty_next_1w'] = reg.predict(test_h1[features])\n",
        "\n",
        "# Horizon 2 Predictions\n",
        "submission['Target_purchase_next_2w'] = clf.predict_proba(test_h2[features])[:, 1]\n",
        "submission['Target_qty_next_2w'] = reg.predict(test_h2[features])\n",
        "\n",
        "# Final Cleanup (No negative quantities)\n",
        "submission['Target_qty_next_1w'] = submission['Target_qty_next_1w'].clip(lower=0)\n",
        "submission['Target_qty_next_2w'] = submission['Target_qty_next_2w'].clip(lower=0)\n",
        "\n",
        "submission.to_csv('submission_horizon_promax.csv', index=False)\n",
        "print(\"Done! Saved as submission_horizon_promax.csv\")"
      ],
      "metadata": {
        "id": "aBPu2wyiqCkM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "560c5ddf-badd-4ef8-a01a-69acbd1d35a8"
      },
      "id": "aBPu2wyiqCkM",
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Building Full History...\n",
            "Generating lags...\n",
            "Generating Targets...\n",
            "Stacking Data...\n",
            "Training Unified Models...\n",
            "Classifier Trained.\n",
            "Regressor Trained.\n",
            "Generating Predictions...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 7. Validation & Modeling (With Scores!) ---\n",
        "print(\"--- Starting Validation ---\")\n",
        "\n",
        "# A. Create a Time-Based Split for Validation\n",
        "# We use the last 4 weeks of the training data to check our score\n",
        "weeks = sorted(train_stacked['week_start'].unique())\n",
        "val_start_week = weeks[-4]\n",
        "\n",
        "train_subset = train_stacked[train_stacked['week_start'] < val_start_week]\n",
        "val_subset = train_stacked[train_stacked['week_start'] >= val_start_week]\n",
        "\n",
        "# B. Train & Score Classifier (AUC)\n",
        "print(\"Validating Classifier...\")\n",
        "clf_val = lgb.LGBMClassifier(\n",
        "    n_estimators=2000,\n",
        "    learning_rate=0.02,\n",
        "    num_leaves=40,\n",
        "    random_state=42,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8\n",
        ")\n",
        "clf_val.fit(\n",
        "    train_subset[features],\n",
        "    train_subset['target_buy'],\n",
        "    eval_set=[(val_subset[features], val_subset['target_buy'])],\n",
        "    eval_metric='auc',\n",
        "    callbacks=[lgb.early_stopping(stopping_rounds=100)]\n",
        ")\n",
        "\n",
        "# Calculate Validation AUC\n",
        "val_preds_prob = clf_val.predict_proba(val_subset[features])[:, 1]\n",
        "from sklearn.metrics import roc_auc_score, mean_absolute_error\n",
        "val_auc = roc_auc_score(val_subset['target_buy'], val_preds_prob)\n",
        "print(f\"✅ LOCAL VALIDATION AUC: {val_auc:.5f}\")\n",
        "\n",
        "# C. Train & Score Regressor (MAE)\n",
        "print(\"Validating Regressor...\")\n",
        "reg_val = lgb.LGBMRegressor(\n",
        "    objective='tweedie',\n",
        "    tweedie_variance_power=1.5,\n",
        "    n_estimators=2000,\n",
        "    learning_rate=0.02,\n",
        "    num_leaves=40,\n",
        "    random_state=42,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8\n",
        ")\n",
        "reg_val.fit(\n",
        "    train_subset[features],\n",
        "    train_subset['target_qty'],\n",
        "    eval_set=[(val_subset[features], val_subset['target_qty'])],\n",
        "    eval_metric='mae',\n",
        "    callbacks=[lgb.early_stopping(stopping_rounds=100)]\n",
        ")\n",
        "\n",
        "# Calculate Validation MAE\n",
        "val_preds_qty = reg_val.predict(val_subset[features])\n",
        "val_mae = mean_absolute_error(val_subset['target_qty'], val_preds_qty)\n",
        "print(f\"✅ LOCAL VALIDATION MAE: {val_mae:.5f}\")\n",
        "\n",
        "# --- 8. Final Retraining & Prediction ---\n",
        "print(\"\\n--- Retraining on FULL Dataset for Submission ---\")\n",
        "# Now that we know the score, we train on EVERYTHING (Train + Validation) to get max performance\n",
        "clf_full = lgb.LGBMClassifier(\n",
        "    n_estimators=2500,\n",
        "    learning_rate=0.02,\n",
        "    num_leaves=40,\n",
        "    random_state=42,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8\n",
        ")\n",
        "clf_full.fit(train_stacked[features], train_stacked['target_buy'])\n",
        "\n",
        "reg_full = lgb.LGBMRegressor(\n",
        "    objective='tweedie',\n",
        "    tweedie_variance_power=1.5,\n",
        "    n_estimators=2500,\n",
        "    learning_rate=0.02,\n",
        "    num_leaves=40,\n",
        "    random_state=42,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8\n",
        ")\n",
        "reg_full.fit(train_stacked[features], train_stacked['target_qty'])\n",
        "\n",
        "print(\"Generating Final Predictions...\")\n",
        "\n",
        "# Prepare Test sets for both horizons\n",
        "test_h1 = test_df_source.copy()\n",
        "test_h1['horizon'] = 1\n",
        "\n",
        "test_h2 = test_df_source.copy()\n",
        "test_h2['horizon'] = 2\n",
        "\n",
        "submission = test[['ID']].copy()\n",
        "\n",
        "# Horizon 1 Predictions\n",
        "submission['Target_purchase_next_1w'] = clf_full.predict_proba(test_h1[features])[:, 1]\n",
        "submission['Target_qty_next_1w'] = reg_full.predict(test_h1[features])\n",
        "\n",
        "# Horizon 2 Predictions\n",
        "submission['Target_purchase_next_2w'] = clf_full.predict_proba(test_h2[features])[:, 1]\n",
        "submission['Target_qty_next_2w'] = reg_full.predict(test_h2[features])\n",
        "\n",
        "# Final Cleanup (No negative quantities)\n",
        "submission['Target_qty_next_1w'] = submission['Target_qty_next_1w'].clip(lower=0)\n",
        "submission['Target_qty_next_2w'] = submission['Target_qty_next_2w'].clip(lower=0)\n",
        "\n",
        "submission.to_csv('submission_horizon_promax_scored.csv', index=False)\n",
        "print(\"Done! Saved as submission_horizon_promax_scored.csv\")"
      ],
      "metadata": {
        "id": "Kw93kEuMLCoJ"
      },
      "id": "Kw93kEuMLCoJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ADD THIS TO YOUR FEATURE ENGINEERING STEP ---\n",
        "\n",
        "# 1. Extract Date Features\n",
        "# These help the model learn seasonality (e.g., mango season)\n",
        "train['month'] = train['week_start'].dt.month\n",
        "train['week_of_year'] = train['week_start'].dt.isocalendar().week.astype(int)\n",
        "train['quarter'] = train['week_start'].dt.quarter\n",
        "\n",
        "test['month'] = test['week_start'].dt.month\n",
        "test['week_of_year'] = test['week_start'].dt.isocalendar().week.astype(int)\n",
        "test['quarter'] = test['week_start'].dt.quarter\n",
        "\n",
        "# 2. Calculate Customer Tenure (Days on Platform)\n",
        "# Older customers behave differently than new ones\n",
        "train['tenure_days'] = (train['week_start'] - train['customer_created_at']).dt.days\n",
        "test['tenure_days'] = (test['week_start'] - test['customer_created_at']).dt.days\n",
        "\n",
        "# 3. Add these new columns to your 'features' list\n",
        "# (Update the list in the Modeling Step)\n",
        "# features = ['month', 'week_of_year', 'tenure_days', 'customer_category', ...] + feature_cols"
      ],
      "metadata": {
        "id": "peksEIM2X-bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 845
        },
        "collapsed": true,
        "outputId": "9178bd6d-b41e-4b28-adc0-eaae5f1d1983"
      },
      "id": "peksEIM2X-bc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "cannot convert NA to integer",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1709773297.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# These help the model learn seasonality (e.g., mango season)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'month'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'week_start'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'week_of_year'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'week_start'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misocalendar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweek\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'quarter'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'week_start'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquarter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   6641\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6642\u001b[0m             \u001b[0;31m# else, only a single dtype is given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6643\u001b[0;31m             \u001b[0mnew_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6644\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor_from_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6645\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"astype\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m         return self.apply(\n\u001b[0m\u001b[1;32m    431\u001b[0m             \u001b[0;34m\"astype\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m                 \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextend_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors, using_cow, squeeze)\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# type: ignore[call-overload]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mastype_array_safe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_coerce_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/dtypes/astype.py\u001b[0m in \u001b[0;36mastype_array_safe\u001b[0;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mastype_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;31m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/dtypes/astype.py\u001b[0m in \u001b[0;36mastype_array\u001b[0;34m(values, dtype, copy)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;31m# i.e. ExtensionArray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/arrays/masked.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;31m# to_numpy will also raise, but we get somewhat nicer exception messages here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m\"iu\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hasna\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cannot convert NA to integer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hasna\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;31m# careful: astype_nansafe converts np.nan to True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot convert NA to integer"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "import gc\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import roc_auc_score, mean_absolute_error\n",
        "\n",
        "# --- 1. Load Data ---\n",
        "print(\"Loading data...\")\n",
        "train = pd.read_csv('Train.csv')\n",
        "test = pd.read_csv('Test.csv')\n",
        "customer = pd.read_csv('customer_data.csv')\n",
        "sku = pd.read_csv('sku_data.csv')\n",
        "\n",
        "# --- 2. Downcast (Keep Memory Low) ---\n",
        "def downcast(df):\n",
        "    for col in df.select_dtypes(include=['float64']).columns:\n",
        "        df[col] = df[col].astype('float32')\n",
        "    for col in df.select_dtypes(include=['int64']).columns:\n",
        "        if df[col].max() < 2**32:\n",
        "            df[col] = df[col].astype('int32')\n",
        "    return df\n",
        "\n",
        "train = downcast(train)\n",
        "test = downcast(test)\n",
        "customer = downcast(customer)\n",
        "sku = downcast(sku)\n",
        "\n",
        "# Date Conversion\n",
        "train['week_start'] = pd.to_datetime(train['week_start'])\n",
        "test['week_start'] = pd.to_datetime(test['week_start'])\n",
        "customer['customer_created_at'] = pd.to_datetime(customer['customer_created_at'])\n",
        "\n",
        "# --- 3. Feature Engineering (Unified Process) ---\n",
        "print(\"Building Full History and Features...\")\n",
        "\n",
        "# A. Create a base full_df with essential columns for timeline and qty\n",
        "full_df = pd.concat([\n",
        "    train[['customer_id', 'product_unit_variant_id', 'week_start', 'qty_this_week',\n",
        "           'num_orders_week', 'spend_this_week', 'purchased_this_week']], # Train-specific cols\n",
        "    test[['customer_id', 'product_unit_variant_id', 'week_start']].assign(qty_this_week=0.0,\n",
        "           num_orders_week=0.0, spend_this_week=0.0, purchased_this_week=0.0) # Test with placeholder zeros\n",
        "], ignore_index=True)\n",
        "full_df = full_df.sort_values(['customer_id', 'product_unit_variant_id', 'week_start'])\n",
        "\n",
        "# B. Merge Customer and SKU data into full_df\n",
        "full_df = full_df.merge(customer[['customer_id', 'customer_category', 'customer_status', 'customer_created_at']],\n",
        "                        on='customer_id', how='left')\n",
        "full_df = full_df.merge(sku[['product_unit_variant_id', 'grade_name', 'unit_name']],\n",
        "                        on='product_unit_variant_id', how='left')\n",
        "\n",
        "# C. Add Seasonality (The Missing Piece)\n",
        "full_df['month'] = full_df['week_start'].dt.month.fillna(0).astype(int)\n",
        "full_df['week_of_year'] = full_df['week_start'].dt.isocalendar().week.fillna(0).astype(int)\n",
        "\n",
        "# D. Calculate Lags\n",
        "print(\"Generating Lags...\")\n",
        "grp = full_df.groupby(['customer_id', 'product_unit_variant_id'])['qty_this_week']\n",
        "full_df['lag1'] = grp.shift(1)\n",
        "full_df['lag2'] = grp.shift(2)\n",
        "full_df['lag3'] = grp.shift(3)\n",
        "full_df['roll_mean_4'] = grp.rolling(4).mean().reset_index(level=[0,1], drop=True)\n",
        "\n",
        "# E. Global Trend (Calculated STRICTLY on Train to prevent Leakage)\n",
        "print(\"Generating Global Trends...\")\n",
        "# Calculate average sales per product per week in TRAIN only (where qty_this_week > 0)\n",
        "train_only_for_global = full_df[full_df['qty_this_week'] > 0.0].copy()\n",
        "global_stats = train_only_for_global.groupby(['product_unit_variant_id', 'week_start'])['qty_this_week'].mean().reset_index()\n",
        "global_stats.rename(columns={'qty_this_week': 'global_avg_qty'}, inplace=True)\n",
        "\n",
        "# Merge this back to full_df, but SHIFTED by 1 week to represent prior week's trend\n",
        "global_stats['week_start'] = global_stats['week_start'] + pd.Timedelta(weeks=1)\n",
        "full_df = full_df.merge(global_stats, on=['product_unit_variant_id', 'week_start'], how='left')\n",
        "full_df['global_avg_qty'] = full_df['global_avg_qty'].fillna(0)\n",
        "\n",
        "# F. Customer Tenure\n",
        "full_df['tenure_days'] = (full_df['week_start'] - full_df['customer_created_at']).dt.days\n",
        "\n",
        "# G. Fill NaNs for numerical features\n",
        "cols_to_fill = ['lag1', 'lag2', 'lag3', 'roll_mean_4', 'global_avg_qty', 'tenure_days']\n",
        "full_df[cols_to_fill] = full_df[cols_to_fill].fillna(0)\n",
        "\n",
        "# H. Encode Categoricals (after all merges and before splitting)\n",
        "cat_cols = ['customer_category', 'customer_status', 'grade_name', 'unit_name']\n",
        "le = LabelEncoder()\n",
        "for col in cat_cols:\n",
        "    full_df[col] = full_df[col].astype(str).fillna('UNKNOWN')\n",
        "    full_df[col] = le.fit_transform(full_df[col])\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "# --- 4. Prepare Train/Test for Modeling (Split Enriched full_df) ---\n",
        "print(\"Preparing Train/Test sets...\")\n",
        "\n",
        "# Get original train IDs and week_starts\n",
        "original_train_ids = train[['customer_id', 'product_unit_variant_id', 'week_start']].drop_duplicates()\n",
        "\n",
        "# Filter full_df to get the enriched train and test sets\n",
        "train_enriched = pd.merge(original_train_ids, full_df, on=['customer_id', 'product_unit_variant_id', 'week_start'], how='left')\n",
        "test_enriched = full_df[~full_df.set_index(['customer_id', 'product_unit_variant_id', 'week_start']).index.isin(\n",
        "    original_train_ids.set_index(['customer_id', 'product_unit_variant_id', 'week_start']).index\n",
        ")].copy()\n",
        "\n",
        "# Ensure 'ID' column is restored for test_enriched from the original test dataframe\n",
        "test_enriched = pd.merge(test[['ID', 'customer_id', 'product_unit_variant_id', 'week_start']],\n",
        "                         test_enriched,\n",
        "                         on=['customer_id', 'product_unit_variant_id', 'week_start'],\n",
        "                         how='left')\n",
        "\n",
        "# Overwrite original train/test with enriched versions\n",
        "train = train_enriched\n",
        "test = test_enriched\n",
        "\n",
        "# --- 5. Target Generation (The \"Wide\" Way) ---\n",
        "print(\"Generating Targets...\")\n",
        "train = train.sort_values(['customer_id', 'product_unit_variant_id', 'week_start'])\n",
        "grp = train.groupby(['customer_id', 'product_unit_variant_id'])['qty_this_week']\n",
        "train['target_qty_1w'] = grp.shift(-1).fillna(0)\n",
        "train['target_qty_2w'] = grp.shift(-2).fillna(0)\n",
        "train['target_buy_1w'] = (train['target_qty_1w'] > 0).astype(int)\n",
        "train['target_buy_2w'] = (train['target_qty_2w'] > 0).astype(int)\n",
        "\n",
        "# Drop rows where targets are NaN (end of history for target calculation)\n",
        "train = train.dropna(subset=['target_qty_1w', 'target_qty_2w'])\n",
        "\n",
        "# --- 6. Modeling (4 Separate Models - The Stable Way) ---\n",
        "print(\"Training Models...\")\n",
        "\n",
        "# Features list\n",
        "features = cols_to_fill + ['month', 'week_of_year'] + cat_cols + \\\n",
        "           ['num_orders_week', 'spend_this_week', 'purchased_this_week'] # Add original train specific columns if desired\n",
        "\n",
        "# Filter features to only include those present in the DataFrame (e.g. if num_orders_week etc were not merged properly)\n",
        "features = [f for f in features if f in train.columns and f in test.columns]\n",
        "\n",
        "# Validation Split (Last 4 weeks)\n",
        "weeks = sorted(train['week_start'].unique())\n",
        "val_weeks = weeks[-4:]\n",
        "val_mask = train['week_start'].isin(val_weeks)\n",
        "\n",
        "X_train = train.loc[~val_mask, features]\n",
        "X_val = train.loc[val_mask, features]\n",
        "\n",
        "models = {}\n",
        "\n",
        "# 1. Week 1 Classifier\n",
        "print(\"Training Week 1 Classifier...\")\n",
        "clf1 = lgb.LGBMClassifier(n_estimators=1500, learning_rate=0.03, random_state=42, verbose=-1)\n",
        "clf1.fit(X_train, train.loc[~val_mask, 'target_buy_1w'],\n",
        "         eval_set=[(X_val, train.loc[val_mask, 'target_buy_1w'])], eval_metric='auc',\n",
        "         callbacks=[lgb.early_stopping(50)])\n",
        "models['p1'] = clf1\n",
        "\n",
        "# 2. Week 1 Regressor (Tweedie)\n",
        "print(\"Training Week 1 Regressor...\")\n",
        "reg1 = lgb.LGBMRegressor(objective='tweedie', tweedie_variance_power=1.5, n_estimators=1500, learning_rate=0.03, random_state=42, verbose=-1)\n",
        "reg1.fit(X_train, train.loc[~val_mask, 'target_qty_1w'],\n",
        "         eval_set=[(X_val, train.loc[val_mask, 'target_qty_1w'])], eval_metric='mae',\n",
        "         callbacks=[lgb.early_stopping(50)])\n",
        "models['q1'] = reg1\n",
        "\n",
        "# 3. Week 2 Classifier\n",
        "print(\"Training Week 2 Classifier...\")\n",
        "clf2 = lgb.LGBMClassifier(n_estimators=1500, learning_rate=0.03, random_state=42, verbose=-1)\n",
        "clf2.fit(X_train, train.loc[~val_mask, 'target_buy_2w'],\n",
        "         eval_set=[(X_val, train.loc[val_mask, 'target_buy_2w'])], eval_metric='auc',\n",
        "         callbacks=[lgb.early_stopping(50)])\n",
        "models['p2'] = clf2\n",
        "\n",
        "# 4. Week 2 Regressor (Tweedie)\n",
        "print(\"Training Week 2 Regressor...\")\n",
        "reg2 = lgb.LGBMRegressor(objective='tweedie', tweedie_variance_power=1.5, n_estimators=1500, learning_rate=0.03, random_state=42, verbose=-1)\n",
        "reg2.fit(X_train, train.loc[~val_mask, 'target_qty_2w'],\n",
        "         eval_set=[(X_val, train.loc[val_mask, 'target_qty_2w'])], eval_metric='mae',\n",
        "         callbacks=[lgb.early_stopping(50)])\n",
        "models['q2'] = reg2\n",
        "\n",
        "# --- 7. Submission ---\n",
        "print(\"Generating Predictions...\")\n",
        "submission = test[['ID']].copy() # Use the original test ID column\n",
        "\n",
        "submission['Target_purchase_next_1w'] = models['p1'].predict_proba(test[features])[:, 1]\n",
        "submission['Target_qty_next_1w'] = models['q1'].predict(test[features])\n",
        "submission['Target_purchase_next_2w'] = models['p2'].predict_proba(test[features])[:, 1]\n",
        "submission['Target_qty_next_2w'] = models['q2'].predict(test[features])\n",
        "\n",
        "# Clip negatives\n",
        "submission['Target_qty_next_1w'] = submission['Target_qty_next_1w'].clip(lower=0)\n",
        "submission['Target_qty_next_2w'] = submission['Target_qty_next_2w'].clip(lower=0)\n",
        "\n",
        "submission.to_csv('submission_safe_upgrade.csv', index=False)\n",
        "print(\"Done! Safe upgrade saved.\")"
      ],
      "metadata": {
        "id": "jrsU6T1Sx-Bq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a0de758-3a0c-4c95-ced9-0e3f262c555f"
      },
      "id": "jrsU6T1Sx-Bq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Building Full History and Features...\n",
            "Generating Lags...\n",
            "Generating Global Trends...\n",
            "Preparing Train/Test sets...\n",
            "Generating Targets...\n",
            "Training Models...\n",
            "Training Week 1 Classifier...\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[61]\tvalid_0's auc: 0.951647\tvalid_0's binary_logloss: 0.0534513\n",
            "Training Week 1 Regressor...\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[18]\tvalid_0's l1: 1.49447\tvalid_0's tweedie: 3.89564\n",
            "Training Week 2 Classifier...\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[31]\tvalid_0's auc: 0.936496\tvalid_0's binary_logloss: 0.0504207\n",
            "Training Week 2 Regressor...\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[75]\tvalid_0's l1: 1.14246\tvalid_0's tweedie: 1.55625\n",
            "Generating Predictions...\n",
            "Done! Safe upgrade saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your latest submission\n",
        "sub = pd.read_csv('submission_safe_upgrade.csv')\n",
        "\n",
        "# Multiply Quantity * Probability\n",
        "# If Prob is 0.1 and Qty is 10, the \"Expected\" Qty is 1.\n",
        "sub['Target_qty_next_1w'] = sub['Target_qty_next_1w'] * sub['Target_purchase_next_1w']\n",
        "sub['Target_qty_next_2w'] = sub['Target_qty_next_2w'] * sub['Target_purchase_next_2w']\n",
        "\n",
        "# Save as a new version\n",
        "sub.to_csv('submission_expected_value.csv', index=False)\n",
        "print(\"Applied Expected Value correction.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzETKtJRHJN-",
        "outputId": "cd0ff498-ea35-44a1-c814-a88cab807a20"
      },
      "id": "FzETKtJRHJN-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applied Expected Value correction.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Limport pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "import gc\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# --- 1. Load Data ---\n",
        "print(\"Loading data...\")\n",
        "train = pd.read_csv('Train.csv')\n",
        "test = pd.read_csv('Test.csv')\n",
        "customer = pd.read_csv('customer_data.csv')\n",
        "sku = pd.read_csv('sku_data.csv')\n",
        "\n",
        "# --- 2. Downcast ---\n",
        "def downcast(df):\n",
        "    for col in df.select_dtypes(include=['float64']).columns:\n",
        "        df[col] = df[col].astype('float32')\n",
        "    for col in df.select_dtypes(include=['int64']).columns:\n",
        "        if df[col].max() < 2**32:\n",
        "            df[col] = df[col].astype('int32')\n",
        "    return df\n",
        "\n",
        "train = downcast(train)\n",
        "test = downcast(test)\n",
        "customer = downcast(customer)\n",
        "sku = downcast(sku)\n",
        "\n",
        "train['week_start'] = pd.to_datetime(train['week_start'])\n",
        "test['week_start'] = pd.to_datetime(test['week_start'])\n",
        "customer['customer_created_at'] = pd.to_datetime(customer['customer_created_at'])\n",
        "\n",
        "# --- 3. LEAK-FREE Feature Engineering ---\n",
        "print(\"Generating Leak-Free Features...\")\n",
        "\n",
        "# A. Create Full Timeline (Train + Test)\n",
        "# This is crucial so rolling windows flow naturally from Train into Test\n",
        "full_df = pd.concat([\n",
        "    train[['customer_id', 'product_unit_variant_id', 'week_start', 'qty_this_week']],\n",
        "    test[['customer_id', 'product_unit_variant_id', 'week_start']].assign(qty_this_week=0.0) # FIXED: Use 0.0 instead of np.nan\n",
        "], ignore_index=True)\n",
        "\n",
        "full_df = full_df.sort_values(['customer_id', 'product_unit_variant_id', 'week_start'])\n",
        "\n",
        "# B. Individual Lag Features\n",
        "# \"What did THIS customer buy recently?\"\n",
        "grp = full_df.groupby(['customer_id', 'product_unit_variant_id'])['qty_this_week']\n",
        "full_df['lag1'] = grp.shift(1) # Last week\n",
        "full_df['lag2'] = grp.shift(2) # 2 weeks ago\n",
        "full_df['roll_mean_4'] = grp.shift(1).rolling(4).mean() # Avg of last 4 weeks (excluding current)\n",
        "\n",
        "# C. Global Product Trend (The \"Viral\" Factor)\n",
        "# \"How much is this product selling generally?\"\n",
        "# We calculate this on the WHOLE dataset using transform, but we SHIFT first.\n",
        "# This ensures Week 10 only knows about the global volume of Weeks 1-9.\n",
        "# Note: We fillna(0) temporarily for the calculation but keep original logic\n",
        "temp_df = full_df.copy()\n",
        "temp_df['qty_this_week'] = temp_df['qty_this_week'].fillna(0)\n",
        "\n",
        "# Global trend per product per week\n",
        "global_trend = temp_df.groupby(['product_unit_variant_id', 'week_start'])['qty_this_week'].mean().reset_index().rename(columns={'qty_this_week': 'daily_global_vol'})\n",
        "full_df = full_df.merge(global_trend, on=['product_unit_variant_id', 'week_start'], how='left')\n",
        "\n",
        "# Now calculate the LAGGED global trend (Leak-Free)\n",
        "grp_global = full_df.groupby('product_unit_variant_id')['daily_global_vol']\n",
        "full_df['global_lag1'] = grp_global.shift(1)\n",
        "full_df['global_roll_4'] = grp_global.shift(1).rolling(4).mean()\n",
        "\n",
        "# D. Seasonality\n",
        "full_df['month'] = full_df['week_start'].dt.month.fillna(0).astype(int) # Fixed: fillna(0) added\n",
        "full_df['week_of_year'] = full_df['week_start'].dt.isocalendar().week.fillna(0).astype(int) # Fixed: fillna(0) added\n",
        "\n",
        "# --- 4. Merge Metadata ---\n",
        "full_df = full_df.merge(customer, on='customer_id', how='left')\n",
        "full_df = full_df.merge(sku, on='product_unit_variant_id', how='left')\n",
        "\n",
        "# Encode\n",
        "cat_cols = ['customer_category', 'customer_status', 'grade_name', 'unit_name']\n",
        "le = LabelEncoder()\n",
        "for col in cat_cols:\n",
        "    full_df[col] = full_df[col].astype(str).fillna('UNKNOWN')\n",
        "    full_df[col] = le.fit_transform(full_df[col])\n",
        "\n",
        "# Fill Numerical NaNs\n",
        "num_cols = ['lag1', 'lag2', 'roll_mean_4', 'global_lag1', 'global_roll_4']\n",
        "full_df[num_cols] = full_df[num_cols].fillna(0)\n",
        "\n",
        "# --- 5. Target Generation ---\n",
        "print(\"Generating Targets...\")\n",
        "# We use the full_df to shift, then split\n",
        "full_df = full_df.sort_values(['customer_id', 'product_unit_variant_id', 'week_start'])\n",
        "grp = full_df.groupby(['customer_id', 'product_unit_variant_id'])['qty_this_week']\n",
        "\n",
        "full_df['target_qty_1w'] = grp.shift(-1)\n",
        "full_df['target_qty_2w'] = grp.shift(-2)\n",
        "full_df['target_buy_1w'] = (full_df['target_qty_1w'] > 0).astype(float) # Float for probability\n",
        "full_df['target_buy_2w'] = (full_df['target_qty_2w'] > 0).astype(float)\n",
        "\n",
        "# --- 6. Split Train/Test ---\n",
        "# Filter back to original Train set rows (where we have actuals) and Test set rows\n",
        "train_df = full_df[full_df['week_start'].isin(train['week_start'])].copy()\n",
        "test_df = full_df[full_df['week_start'].isin(test['week_start'])].copy()\n",
        "\n",
        "# Remove rows in Train that don't have valid targets (the very last weeks of history)\n",
        "train_df = train_df.dropna(subset=['target_qty_1w']) # For 1w model\n",
        "train_df_2w = train_df.dropna(subset=['target_qty_2w']) # For 2w model\n",
        "\n",
        "# --- 7. Modeling ---\n",
        "print(\"Training Models...\")\n",
        "feature_cols = ['lag1', 'lag2', 'roll_mean_4', 'global_lag1', 'global_roll_4',\n",
        "                'month', 'week_of_year'] + cat_cols\n",
        "\n",
        "models = {}\n",
        "\n",
        "# Week 1 Classifier\n",
        "print(\"  Week 1 Classifier...\")\n",
        "clf1 = lgb.LGBMClassifier(n_estimators=2000, learning_rate=0.03, num_leaves=31, random_state=42)\n",
        "clf1.fit(train_df[feature_cols], train_df['target_buy_1w'])\n",
        "\n",
        "# Filter for non-zero quantities for regression tasks\n",
        "train_df_qty1 = train_df[train_df['target_qty_1w'] > 0].copy()\n",
        "train_df_qty2 = train_df_2w[train_df_2w['target_qty_2w'] > 0].copy()\n",
        "\n",
        "# Week 1 Regressor (Tweedie)\n",
        "print(\"  Week 1 Regressor...\")\n",
        "reg1 = lgb.LGBMRegressor(objective='tweedie', tweedie_variance_power=1.5, n_estimators=2000, learning_rate=0.03, random_state=42)\n",
        "# Only train on rows where target_qty_1w is greater than 0\n",
        "if not train_df_qty1.empty:\n",
        "    reg1.fit(train_df_qty1[feature_cols], train_df_qty1['target_qty_1w'])\n",
        "else:\n",
        "    print(\"Warning: train_df_qty1 is empty. Regressor 1 will not be trained.\")\n",
        "\n",
        "# Week 2 Classifier\n",
        "print(\"  Week 2 Classifier...\")\n",
        "clf2 = lgb.LGBMClassifier(n_estimators=2000, learning_rate=0.03, num_leaves=31, random_state=42)\n",
        "clf2.fit(train_df_2w[feature_cols], train_df_2w['target_buy_2w'])\n",
        "\n",
        "# Week 2 Regressor (Tweedie)\n",
        "print(\"  Week 2 Regressor...\")\n",
        "reg2 = lgb.LGBMRegressor(objective='tweedie', tweedie_variance_power=1.5, n_estimators=2000, learning_rate=0.03, random_state=42)\n",
        "# Only train on rows where target_qty_2w is greater than 0\n",
        "if not train_df_qty2.empty:\n",
        "    reg2.fit(train_df_qty2[feature_cols], train_df_qty2['target_qty_2w'])\n",
        "else:\n",
        "    print(\"Warning: train_df_qty2 is empty. Regressor 2 will not be trained.\")\n",
        "\n",
        "# --- 8. Submission with EV Optimization ---\n",
        "print(\"Generating Final Optimized Predictions...\")\n",
        "submission = test[['ID']].copy()\n",
        "\n",
        "# A. Raw Predictions\n",
        "p1 = clf1.predict_proba(test_df[feature_cols])[:, 1]\n",
        "q1 = reg1.predict(test_df[feature_cols]) if not train_df_qty1.empty else np.zeros(len(test_df))\n",
        "p2 = clf2.predict_proba(test_df[feature_cols])[:, 1]\n",
        "q2 = reg2.predict(test_df[feature_cols]) if not train_df_qty2.empty else np.zeros(len(test_df))\n",
        "\n",
        "# B. Store Raw Probs (For AUC)\n",
        "submission['Target_purchase_next_1w'] = p1\n",
        "submission['Target_purchase_next_2w'] = p2\n",
        "\n",
        "# C. Apply Expected Value Optimization (For MAE)\n",
        "# Qty = Predicted_Qty * Probability_of_Purchase\n",
        "submission['Target_qty_next_1w'] = (q1 * p1).clip(min=0)\n",
        "submission['Target_qty_next_2w'] = (q2 * p2).clip(min=0)\n",
        "\n",
        "submission.to_csv('submission_leak_free_optimized.csv', index=False)\n",
        "print(\"Done! Upload 'submission_leak_free_optimized.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pp85tIIBeEXd",
        "outputId": "8e8fd015-7ae5-4938-8267-3b6ef3b9276b"
      },
      "id": "Pp85tIIBeEXd",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Generating Leak-Free Features...\n",
            "Generating Targets...\n",
            "Training Models...\n",
            "  Week 1 Classifier...\n",
            "  Week 1 Regressor...\n",
            "Warning: train_df_qty1 is empty. Regressor 1 will not be trained.\n",
            "  Week 2 Classifier...\n",
            "  Week 2 Regressor...\n",
            "Warning: train_df_qty2 is empty. Regressor 2 will not be trained.\n",
            "Generating Final Optimized Predictions...\n",
            "Done! Upload 'submission_leak_free_optimized.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "import gc\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# --- 1. Load Data (NO DOWNCASTING) ---\n",
        "print(\"Loading data...\")\n",
        "# We let Pandas choose the safe types (int64/float64) automatically.\n",
        "# This uses more RAM but guarantees 100% data integrity.\n",
        "train = pd.read_csv('Train.csv')\n",
        "test = pd.read_csv('Test.csv')\n",
        "customer = pd.read_csv('customer_data.csv')\n",
        "sku = pd.read_csv('sku_data.csv')\n",
        "\n",
        "# Explicitly convert Dates\n",
        "train['week_start'] = pd.to_datetime(train['week_start'])\n",
        "test['week_start'] = pd.to_datetime(test['week_start'])\n",
        "customer['customer_created_at'] = pd.to_datetime(customer['customer_created_at'])\n",
        "\n",
        "# --- 2. Feature Engineering ---\n",
        "print(\"Generating Features...\")\n",
        "\n",
        "# Create Flag\n",
        "train['is_train'] = 1\n",
        "test['is_train'] = 0\n",
        "\n",
        "# Combine (Safely)\n",
        "# We only take necessary columns to keep RAM manageable\n",
        "cols = ['customer_id', 'product_unit_variant_id', 'week_start', 'qty_this_week', 'is_train']\n",
        "full_df = pd.concat([\n",
        "    train[cols],\n",
        "    test[['customer_id', 'product_unit_variant_id', 'week_start', 'is_train']].assign(qty_this_week=0.0) # FIXED: Use 0.0 for test qty_this_week\n",
        "], ignore_index=True)\n",
        "\n",
        "# Sort (Crucial: IDs must be correct for this to work)\n",
        "full_df = full_df.sort_values(['customer_id', 'product_unit_variant_id', 'week_start'])\n",
        "\n",
        "# --- Lags & Trends ---\n",
        "print(\"  Calculating Rolling Stats...\")\n",
        "grp_obj = full_df.groupby(['customer_id', 'product_unit_variant_id'])['qty_this_week']\n",
        "\n",
        "full_df['lag1'] = grp_obj.shift(1)\n",
        "full_df['lag2'] = grp_obj.shift(2)\n",
        "# transform keeps the index alignment safe\n",
        "full_df['roll_mean_4'] = grp_obj.transform(lambda x: x.shift(1).rolling(4).mean())\n",
        "\n",
        "print(\"  Calculating Global Trends...\")\n",
        "# Global Trend logic\n",
        "temp_df = full_df.copy()\n",
        "temp_df['qty_this_week'] = temp_df['qty_this_week'].fillna(0) # Fill NaN for global calculation only\n",
        "\n",
        "global_trend = temp_df.groupby(['product_unit_variant_id', 'week_start'])['qty_this_week'].mean().reset_index().rename(columns={'qty_this_week': 'daily_global_vol'})\n",
        "full_df = full_df.merge(global_trend, on=['product_unit_variant_id', 'week_start'], how='left')\n",
        "\n",
        "grp_global = full_df.groupby('product_unit_variant_id')['daily_global_vol']\n",
        "full_df['global_lag1'] = grp_global.shift(1)\n",
        "full_df['global_roll_4'] = grp_global.transform(lambda x: x.shift(1).rolling(4).mean())\n",
        "\n",
        "# Seasonality\n",
        "full_df['month'] = full_df['week_start'].dt.month.fillna(0).astype(int) # Fixed: fillna(0) added\n",
        "full_df['week_of_year'] = full_df['week_start'].dt.isocalendar().week.fillna(0).astype(int) # Fixed: fillna(0) added\n",
        "\n",
        "# Cleanup\n",
        "del temp_df, grp_obj, grp_global, global_trend\n",
        "gc.collect()\n",
        "\n",
        "# --- 3. Target Generation ---\n",
        "print(\"Generating Targets...\")\n",
        "# Ensure sort is still perfect\n",
        "full_df = full_df.sort_values(['customer_id', 'product_unit_variant_id', 'week_start'])\n",
        "grp = full_df.groupby(['customer_id', 'product_unit_variant_id'])['qty_this_week']\n",
        "\n",
        "full_df['target_qty_1w'] = grp.shift(-1)\n",
        "full_df['target_qty_2w'] = grp.shift(-2)\n",
        "\n",
        "# --- 4. Merge Metadata ---\n",
        "print(\"Merging Metadata...\")\n",
        "full_df = full_df.merge(customer, on='customer_id', how='left')\n",
        "full_df = full_df.merge(sku, on='product_unit_variant_id', how='left')\n",
        "\n",
        "# Encode Categoricals\n",
        "cat_cols = ['customer_category', 'customer_status', 'grade_name', 'unit_name']\n",
        "le = LabelEncoder()\n",
        "for col in cat_cols:\n",
        "    full_df[col] = full_df[col].astype(str).fillna('UNKNOWN')\n",
        "    full_df[col] = le.fit_transform(full_df[col])\n",
        "\n",
        "# Fill NaNs in Features (Lags/Trends) with 0\n",
        "num_cols = ['lag1', 'lag2', 'roll_mean_4', 'global_lag1', 'global_roll_4']\n",
        "full_df[num_cols] = full_df[num_cols].fillna(0)\n",
        "\n",
        "# --- 5. Split & Prepare ---\n",
        "print(\"Splitting Data...\")\n",
        "train_df = full_df[full_df['is_train'] == 1].copy()\n",
        "test_df = full_df[full_df['is_train'] == 0].copy()\n",
        "\n",
        "# Fix Targets for Training\n",
        "# If the history ends, we assume 0 purchase. This prevents dropping rows.\n",
        "train_df['target_qty_1w'] = train_df['target_qty_1w'].fillna(0)\n",
        "train_df['target_qty_2w'] = train_df['target_qty_2w'].fillna(0)\n",
        "\n",
        "# Generate Binary Targets\n",
        "train_df['target_buy_1w'] = (train_df['target_qty_1w'] > 0).astype(int)\n",
        "train_df['target_buy_2w'] = (train_df['target_qty_2w'] > 0).astype(int)\n",
        "\n",
        "print(f\"✅ Training Rows: {len(train_df)} (Should be > 700,000)\")\n",
        "\n",
        "# --- 6. Modeling ---\n",
        "print(\"Training Models...\")\n",
        "features = ['lag1', 'lag2', 'roll_mean_4', 'global_lag1', 'global_roll_4',\n",
        "            'month', 'week_of_year'] + cat_cols\n",
        "\n",
        "# Define LightGBM Params (Tweedie is Critical)\n",
        "clf_params = {'n_estimators': 1500, 'learning_rate': 0.03, 'random_state': 42, 'verbose': -1}\n",
        "reg_params = {'objective': 'tweedie', 'tweedie_variance_power': 1.5,\n",
        "              'n_estimators': 1500, 'learning_rate': 0.03, 'random_state': 42, 'verbose': -1}\n",
        "\n",
        "models = {}\n",
        "\n",
        "# Week 1\n",
        "print(\"  Week 1 Classifier...\")\n",
        "clf1 = lgb.LGBMClassifier(**clf_params)\n",
        "clf1.fit(train_df[features], train_df['target_buy_1w'])\n",
        "\n",
        "# Filter for non-zero quantities for regression tasks\n",
        "train_df_qty1 = train_df[train_df['target_qty_1w'] > 0].copy()\n",
        "\n",
        "print(\"  Week 1 Regressor...\")\n",
        "reg1 = lgb.LGBMRegressor(**reg_params)\n",
        "# Only train on rows where target_qty_1w is greater than 0\n",
        "if not train_df_qty1.empty:\n",
        "    reg1.fit(train_df_qty1[features], train_df_qty1['target_qty_1w'])\n",
        "else:\n",
        "    print(\"Warning: train_df_qty1 (positive quantities for Regressor 1) is empty. Regressor 1 will not be trained.\")\n",
        "\n",
        "# Week 2\n",
        "print(\"  Week 2 Classifier...\")\n",
        "clf2 = lgb.LGBMClassifier(**clf_params)\n",
        "clf2.fit(train_df[features], train_df['target_buy_2w'])\n",
        "\n",
        "# Filter for non-zero quantities for regression tasks (for 2w targets)\n",
        "train_df_qty2 = train_df[train_df['target_qty_2w'] > 0].copy()\n",
        "\n",
        "print(\"  Week 2 Regressor...\")\n",
        "reg2 = lgb.LGBMRegressor(**reg_params)\n",
        "# Only train on rows where target_qty_2w is greater than 0\n",
        "if not train_df_qty2.empty:\n",
        "    reg2.fit(train_df_qty2[features], train_df_qty2['target_qty_2w'])\n",
        "else:\n",
        "    print(\"Warning: train_df_qty2 (positive quantities for Regressor 2) is empty. Regressor 2 will not be trained.\")\n",
        "\n",
        "# --- 7. Submission ---\n",
        "print(\"Generating Submission...\")\n",
        "submission = test[['ID']].copy()\n",
        "\n",
        "# Raw Predictions\n",
        "p1 = clf1.predict_proba(test_df[features])[:, 1]\n",
        "p2 = clf2.predict_proba(test_df[features])[:, 1]\n",
        "\n",
        "# Predict Quantities\n",
        "q1 = reg1.predict(test_df[features]) if not train_df_qty1.empty else np.zeros(len(test_df))\n",
        "q2 = reg2.predict(test_df[features]) if not train_df_qty2.empty else np.zeros(len(test_df))\n",
        "\n",
        "# Save Raw Probabilities (For AUC)\n",
        "submission['Target_purchase_next_1w'] = p1\n",
        "submission['Target_purchase_next_2w'] = p2\n",
        "\n",
        "# Save Optimized Quantities (For MAE) -> Qty * Prob\n",
        "submission['Target_qty_next_1w'] = np.maximum(0, q1 * p1) # FIXED: Using np.maximum for robustness\n",
        "submission['Target_qty_next_2w'] = np.maximum(0, q2 * p2) # FIXED: Using np.maximum for robustness\n",
        "\n",
        "submission.to_csv('submission_safe_v4.csv', index=False)\n",
        "print(\"Done! Upload 'submission_safe_v4.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGQkWURX2rhT",
        "outputId": "8d5352e8-a430-49d4-82de-f230f35f883e"
      },
      "id": "uGQkWURX2rhT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Generating Features...\n",
            "  Calculating Rolling Stats...\n",
            "  Calculating Global Trends...\n",
            "Generating Targets...\n",
            "Merging Metadata...\n",
            "Splitting Data...\n",
            "✅ Training Rows: 40262 (Should be > 700,000)\n",
            "Training Models...\n",
            "  Week 1 Classifier...\n",
            "  Week 1 Regressor...\n",
            "Warning: train_df_qty1 (positive quantities for Regressor 1) is empty. Regressor 1 will not be trained.\n",
            "  Week 2 Classifier...\n",
            "  Week 2 Regressor...\n",
            "Warning: train_df_qty2 (positive quantities for Regressor 2) is empty. Regressor 2 will not be trained.\n",
            "Generating Submission...\n",
            "Done! Upload 'submission_safe_v4.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "import gc\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# --- 1. Load Data (FRESH & SAFE) ---\n",
        "print(\"Loading data...\")\n",
        "# Read fresh to ensure no previous state corruption\n",
        "train = pd.read_csv('Train.csv')\n",
        "test = pd.read_csv('Test.csv')\n",
        "customer = pd.read_csv('customer_data.csv')\n",
        "sku = pd.read_csv('sku_data.csv')\n",
        "\n",
        "# Verify initial shape\n",
        "print(f\"Original Test Rows: {len(test)}\")\n",
        "\n",
        "# Convert Dates\n",
        "train['week_start'] = pd.to_datetime(train['week_start'])\n",
        "test['week_start'] = pd.to_datetime(test['week_start'])\n",
        "customer['customer_created_at'] = pd.to_datetime(customer['customer_created_at'])\n",
        "\n",
        "# --- 2. Feature Engineering ---\n",
        "print(\"Generating Features...\")\n",
        "\n",
        "# Create Flag\n",
        "train['is_train'] = 1\n",
        "test['is_train'] = 0\n",
        "\n",
        "# Combine (No downcasting, keep IDs safe)\n",
        "cols = ['customer_id', 'product_unit_variant_id', 'week_start', 'qty_this_week', 'is_train']\n",
        "full_df = pd.concat([\n",
        "    train[cols],\n",
        "    test[['customer_id', 'product_unit_variant_id', 'week_start', 'is_train']].assign(qty_this_week=np.nan)\n",
        "], ignore_index=True)\n",
        "\n",
        "# Sort strictly for Lag generation\n",
        "full_df = full_df.sort_values(['customer_id', 'product_unit_variant_id', 'week_start'])\n",
        "\n",
        "# --- Lags & Trends ---\n",
        "print(\"  Calculating Rolling Stats...\")\n",
        "grp_obj = full_df.groupby(['customer_id', 'product_unit_variant_id'])['qty_this_week']\n",
        "\n",
        "full_df['lag1'] = grp_obj.shift(1)\n",
        "full_df['lag2'] = grp_obj.shift(2)\n",
        "# transform keeps the index alignment safe\n",
        "full_df['roll_mean_4'] = grp_obj.transform(lambda x: x.shift(1).rolling(4).mean())\n",
        "\n",
        "print(\"  Calculating Global Trends...\")\n",
        "# Global Trend logic\n",
        "temp_df = full_df.copy()\n",
        "temp_df['qty_this_week'] = temp_df['qty_this_week'].fillna(0) # Fill NaN for global calc\n",
        "\n",
        "global_trend = temp_df.groupby(['product_unit_variant_id', 'week_start'])['qty_this_week'].mean().reset_index().rename(columns={'qty_this_week': 'daily_global_vol'})\n",
        "full_df = full_df.merge(global_trend, on=['product_unit_variant_id', 'week_start'], how='left')\n",
        "\n",
        "grp_global = full_df.groupby('product_unit_variant_id')['daily_global_vol']\n",
        "full_df['global_lag1'] = grp_global.shift(1)\n",
        "full_df['global_roll_4'] = grp_global.transform(lambda x: x.shift(1).rolling(4).mean())\n",
        "\n",
        "# Seasonality\n",
        "full_df['month'] = full_df['week_start'].dt.month.fillna(0).astype(int)\n",
        "full_df['week_of_year'] = full_df['week_start'].dt.isocalendar().week.fillna(0).astype(int)\n",
        "\n",
        "# Cleanup\n",
        "del temp_df, grp_obj, grp_global, global_trend\n",
        "gc.collect()\n",
        "\n",
        "# --- 3. Target Generation ---\n",
        "print(\"Generating Targets...\")\n",
        "# Ensure sort is still perfect\n",
        "full_df = full_df.sort_values(['customer_id', 'product_unit_variant_id', 'week_start'])\n",
        "grp = full_df.groupby(['customer_id', 'product_unit_variant_id'])['qty_this_week']\n",
        "\n",
        "full_df['target_qty_1w'] = grp.shift(-1)\n",
        "full_df['target_qty_2w'] = grp.shift(-2)\n",
        "\n",
        "# --- 4. Merge Metadata ---\n",
        "print(\"Merging Metadata...\")\n",
        "full_df = full_df.merge(customer, on='customer_id', how='left')\n",
        "full_df = full_df.merge(sku, on='product_unit_variant_id', how='left')\n",
        "\n",
        "# Encode Categoricals\n",
        "cat_cols = ['customer_category', 'customer_status', 'grade_name', 'unit_name']\n",
        "le = LabelEncoder()\n",
        "for col in cat_cols:\n",
        "    full_df[col] = full_df[col].astype(str).fillna('UNKNOWN')\n",
        "    full_df[col] = le.fit_transform(full_df[col])\n",
        "\n",
        "# Fill NaNs in Features with 0\n",
        "num_cols = ['lag1', 'lag2', 'roll_mean_4', 'global_lag1', 'global_roll_4']\n",
        "full_df[num_cols] = full_df[num_cols].fillna(0)\n",
        "\n",
        "# --- 5. Split & Prepare ---\n",
        "print(\"Splitting Data...\")\n",
        "train_df = full_df[full_df['is_train'] == 1].copy()\n",
        "test_df = full_df[full_df['is_train'] == 0].copy()\n",
        "\n",
        "# Ensure Test is aligned\n",
        "test_df = test_df.sort_values(['ID'] if 'ID' in test_df.columns else ['customer_id', 'product_unit_variant_id'])\n",
        "\n",
        "# Fix Targets for Training\n",
        "train_df['target_qty_1w'] = train_df['target_qty_1w'].fillna(0)\n",
        "train_df['target_qty_2w'] = train_df['target_qty_2w'].fillna(0)\n",
        "\n",
        "# Generate Binary Targets\n",
        "train_df['target_buy_1w'] = (train_df['target_qty_1w'] > 0).astype(int)\n",
        "train_df['target_buy_2w'] = (train_df['target_qty_2w'] > 0).astype(int)\n",
        "\n",
        "print(f\"✅ Training Rows: {len(train_df)}\")\n",
        "print(f\"✅ Test Rows: {len(test_df)} (Should match Original: {len(test)})\")\n",
        "\n",
        "# --- 6. Modeling ---\n",
        "print(\"Training Models...\")\n",
        "features = ['lag1', 'lag2', 'roll_mean_4', 'global_lag1', 'global_roll_4',\n",
        "            'month', 'week_of_year'] + cat_cols\n",
        "\n",
        "# Params\n",
        "clf_params = {'n_estimators': 1500, 'learning_rate': 0.03, 'random_state': 42, 'verbose': -1}\n",
        "# Tweedie is designed for data with many zeros. Do NOT filter zeros out.\n",
        "reg_params = {'objective': 'tweedie', 'tweedie_variance_power': 1.5,\n",
        "              'n_estimators': 1500, 'learning_rate': 0.03, 'random_state': 42, 'verbose': -1}\n",
        "\n",
        "models = {}\n",
        "\n",
        "# Week 1\n",
        "print(\"  Week 1 Classifier...\")\n",
        "clf1 = lgb.LGBMClassifier(**clf_params)\n",
        "clf1.fit(train_df[features], train_df['target_buy_1w'])\n",
        "\n",
        "print(\"  Week 1 Regressor...\")\n",
        "reg1 = lgb.LGBMRegressor(**reg_params)\n",
        "reg1.fit(train_df[features], train_df['target_qty_1w'])\n",
        "\n",
        "# Week 2\n",
        "print(\"  Week 2 Classifier...\")\n",
        "clf2 = lgb.LGBMClassifier(**clf_params)\n",
        "clf2.fit(train_df[features], train_df['target_buy_2w'])\n",
        "\n",
        "print(\"  Week 2 Regressor...\")\n",
        "reg2 = lgb.LGBMRegressor(**reg_params)\n",
        "reg2.fit(train_df[features], train_df['target_qty_2w'])\n",
        "\n",
        "# --- 7. Submission ---\n",
        "print(\"Generating Submission...\")\n",
        "\n",
        "# Ensure we use the exact Test IDs from the source file to prevent \"Missing Entries\"\n",
        "submission = pd.read_csv('Test.csv')[['ID']]\n",
        "\n",
        "# We must align predictions to this submission ID order\n",
        "# We merge the predictions onto the submission DataFrame to guarantee alignment\n",
        "preds_df = test_df[['customer_id', 'product_unit_variant_id', 'week_start']].copy()\n",
        "\n",
        "# Raw Predictions\n",
        "preds_df['p1'] = clf1.predict_proba(test_df[features])[:, 1]\n",
        "preds_df['q1'] = reg1.predict(test_df[features])\n",
        "preds_df['p2'] = clf2.predict_proba(test_df[features])[:, 1]\n",
        "preds_df['q2'] = reg2.predict(test_df[features])\n",
        "\n",
        "# Re-construct ID for merging to be 100% safe\n",
        "# Format: customer_id_product_id_date (Assuming standard format, but safer to merge on columns)\n",
        "# Actually, Test.csv has the ID. Let's merge on keys.\n",
        "preds_df['ID'] = test['ID'] # Since test_df comes from test and we haven't dropped rows, this aligns.\n",
        "\n",
        "# Merge predictions onto the master submission template\n",
        "submission = submission.merge(preds_df[['ID', 'p1', 'q1', 'p2', 'q2']], on='ID', how='left')\n",
        "\n",
        "# Assign to Target Columns\n",
        "submission['Target_purchase_next_1w'] = submission['p1']\n",
        "submission['Target_purchase_next_2w'] = submission['p2']\n",
        "\n",
        "# Optimization: Qty * Prob\n",
        "submission['Target_qty_next_1w'] = (submission['q1'] * submission['p1']).clip(lower=0)\n",
        "submission['Target_qty_next_2w'] = (submission['q2'] * submission['p2']).clip(lower=0)\n",
        "\n",
        "# Drop temp columns\n",
        "submission = submission[['ID', 'Target_purchase_next_1w', 'Target_qty_next_1w', 'Target_purchase_next_2w', 'Target_qty_next_2w']]\n",
        "\n",
        "# Final Check\n",
        "print(f\"Submission Rows: {len(submission)}\")\n",
        "if submission.isnull().sum().sum() > 0:\n",
        "    print(\"⚠️ WARNING: NaNs found in submission. Filling with 0.\")\n",
        "    submission = submission.fillna(0)\n",
        "\n",
        "submission.to_csv('submission_final_clean.csv', index=False)\n",
        "print(\"Done! Upload 'submission_final_clean.csv'\")"
      ],
      "metadata": {
        "id": "qrF1Q-TWdiro",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70ba8a72-3794-4aa4-ea4d-174bfc223b4e"
      },
      "id": "qrF1Q-TWdiro",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Original Test Rows: 275796\n",
            "Generating Features...\n",
            "  Calculating Rolling Stats...\n",
            "  Calculating Global Trends...\n",
            "Generating Targets...\n",
            "Merging Metadata...\n",
            "Splitting Data...\n",
            "✅ Training Rows: 1690870\n",
            "✅ Test Rows: 275796 (Should match Original: 275796)\n",
            "Training Models...\n",
            "  Week 1 Classifier...\n",
            "  Week 1 Regressor...\n",
            "  Week 2 Classifier...\n",
            "  Week 2 Regressor...\n",
            "Generating Submission...\n",
            "Submission Rows: 275796\n",
            "⚠️ WARNING: NaNs found in submission. Filling with 0.\n",
            "Done! Upload 'submission_final_clean.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- SUBMISSION FIX: Align IDs 100% Correctly ---\n",
        "print(\"FIXING SUBMISSION ALIGNMENT...\")\n",
        "\n",
        "# 1. Reconstruct the ID in test_df from the data columns\n",
        "# Format appears to be: CustomerID_ProductUnitID_YYYYMMDD\n",
        "# We use the columns that are definitely aligned with the predictions\n",
        "test_df['reconstructed_ID'] = (\n",
        "    test_df['customer_id'].astype(str) + '_' +\n",
        "    test_df['product_unit_variant_id'].astype(str) + '_' +\n",
        "    test_df['week_start'].dt.strftime('%Y%m%d')\n",
        ")\n",
        "\n",
        "# 2. Assign the predictions to test_df (which is sorted/shuffled)\n",
        "test_df['p1_pred'] = clf1.predict_proba(test_df[features])[:, 1]\n",
        "test_df['q1_pred'] = reg1.predict(test_df[features])\n",
        "test_df['p2_pred'] = clf2.predict_proba(test_df[features])[:, 1]\n",
        "test_df['q2_pred'] = reg2.predict(test_df[features])\n",
        "\n",
        "# 3. Load the Template to ensure we have the exact Target IDs order\n",
        "template = pd.read_csv('Test.csv')\n",
        "target_ids = template[['ID']].copy()\n",
        "\n",
        "# 4. Merge predictions onto the Template using the Reconstructed ID\n",
        "# This ignores row order and matches strictly by the Unique ID string\n",
        "final_sub = target_ids.merge(\n",
        "    test_df[['reconstructed_ID', 'p1_pred', 'q1_pred', 'p2_pred', 'q2_pred']],\n",
        "    left_on='ID',\n",
        "    right_on='reconstructed_ID',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# 5. Optimization (Expected Value)\n",
        "final_sub['Target_purchase_next_1w'] = final_sub['p1_pred']\n",
        "final_sub['Target_qty_next_1w'] = (final_sub['q1_pred'] * final_sub['p1_pred']).clip(lower=0)\n",
        "final_sub['Target_purchase_next_2w'] = final_sub['p2_pred']\n",
        "final_sub['Target_qty_next_2w'] = (final_sub['q2_pred'] * final_sub['p2_pred']).clip(lower=0)\n",
        "\n",
        "# 6. Final Cleanup\n",
        "submission_file = final_sub[['ID', 'Target_purchase_next_1w', 'Target_qty_next_1w', 'Target_purchase_next_2w', 'Target_qty_next_2w']]\n",
        "\n",
        "# Fill any missing matches with 0 (Safe fallback)\n",
        "submission_file = submission_file.fillna(0)\n",
        "\n",
        "print(f\"Final Submission Shape: {submission_file.shape}\")\n",
        "submission_file.to_csv('submission_aligned_final.csv', index=False)\n",
        "print(\"✅ Done! Upload 'submission_aligned_final.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPhxDP2MnqHx",
        "outputId": "b5ccf872-546b-4b07-bcb6-226e40664a0d"
      },
      "id": "vPhxDP2MnqHx",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FIXING SUBMISSION ALIGNMENT...\n",
            "Final Submission Shape: (275796, 5)\n",
            "✅ Done! Upload 'submission_aligned_final.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "import gc\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# --- 1. Load Data (SAFE MODE) ---\n",
        "print(\"Loading data...\")\n",
        "# We use standard loading. No downcasting. No ID corruption.\n",
        "train = pd.read_csv('Train.csv')\n",
        "test = pd.read_csv('Test.csv')\n",
        "customer = pd.read_csv('customer_data.csv')\n",
        "sku = pd.read_csv('sku_data.csv')\n",
        "\n",
        "# Safe Date Conversion\n",
        "train['week_start'] = pd.to_datetime(train['week_start'])\n",
        "test['week_start'] = pd.to_datetime(test['week_start'])\n",
        "customer['customer_created_at'] = pd.to_datetime(customer['customer_created_at'])\n",
        "\n",
        "# --- 2. Feature Engineering (The Hybrid Approach) ---\n",
        "print(\"Generating Features...\")\n",
        "\n",
        "# We create a temporary 'Universal' dataframe just to calculate the smart features.\n",
        "# We will NOT use this for the final submission structure.\n",
        "cols_needed = ['customer_id', 'product_unit_variant_id', 'week_start', 'qty_this_week']\n",
        "temp_df = pd.concat([\n",
        "    train[cols_needed],\n",
        "    test[['customer_id', 'product_unit_variant_id', 'week_start']].assign(qty_this_week=0.0)\n",
        "], ignore_index=True)\n",
        "\n",
        "# Sort strictly for Lag calculation\n",
        "temp_df = temp_df.sort_values(['customer_id', 'product_unit_variant_id', 'week_start'])\n",
        "\n",
        "# --- CALCULATIONS (The 0.97 AUC Logic) ---\n",
        "\n",
        "print(\"  Calculating Rolling Stats...\")\n",
        "grp_obj = temp_df.groupby(['customer_id', 'product_unit_variant_id'])['qty_this_week']\n",
        "temp_df['lag1'] = grp_obj.shift(1)\n",
        "temp_df['lag2'] = grp_obj.shift(2)\n",
        "# Use .transform to avoid index errors\n",
        "temp_df['roll_mean_4'] = grp_obj.transform(lambda x: x.shift(1).rolling(4).mean())\n",
        "\n",
        "print(\"  Calculating Global Trends...\")\n",
        "# 1. Calculate Global Volume per week\n",
        "global_calc = temp_df.copy()\n",
        "global_trend = global_calc.groupby(['product_unit_variant_id', 'week_start'])['qty_this_week'].mean().reset_index().rename(columns={'qty_this_week': 'daily_global_vol'})\n",
        "\n",
        "# 2. Merge back to temp_df\n",
        "temp_df = temp_df.merge(global_trend, on=['product_unit_variant_id', 'week_start'], how='left')\n",
        "\n",
        "# 3. Calculate Lagged Global Trend (Leak-Free)\n",
        "grp_global = temp_df.groupby('product_unit_variant_id')['daily_global_vol']\n",
        "temp_df['global_lag1'] = grp_global.shift(1)\n",
        "temp_df['global_roll_4'] = grp_global.transform(lambda x: x.shift(1).rolling(4).mean())\n",
        "\n",
        "# Seasonality\n",
        "temp_df['month'] = temp_df['week_start'].dt.month.fillna(0).astype(int)\n",
        "temp_df['week_of_year'] = temp_df['week_start'].dt.isocalendar().week.fillna(0).astype(int)\n",
        "\n",
        "# --- 3. Merge Features Back to SAFE Dataframes ---\n",
        "# This is the critical step. We take the smart numbers and put them into the original, correct row order.\n",
        "print(\"Merging Features back to Train/Test...\")\n",
        "feature_cols = ['lag1', 'lag2', 'roll_mean_4', 'global_lag1', 'global_roll_4', 'month', 'week_of_year']\n",
        "\n",
        "train = train.merge(temp_df[['customer_id', 'product_unit_variant_id', 'week_start'] + feature_cols],\n",
        "                    on=['customer_id', 'product_unit_variant_id', 'week_start'], how='left')\n",
        "\n",
        "test = test.merge(temp_df[['customer_id', 'product_unit_variant_id', 'week_start'] + feature_cols],\n",
        "                  on=['customer_id', 'product_unit_variant_id', 'week_start'], how='left')\n",
        "\n",
        "# Cleanup RAM\n",
        "del temp_df, grp_obj, global_trend, grp_global, global_calc\n",
        "gc.collect()\n",
        "\n",
        "# --- 4. Merge Metadata & Encode ---\n",
        "print(\"Merging Metadata...\")\n",
        "train = train.merge(customer, on='customer_id', how='left')\n",
        "train = train.merge(sku, on='product_unit_variant_id', how='left')\n",
        "test = test.merge(customer, on='customer_id', how='left')\n",
        "test = test.merge(sku, on='product_unit_variant_id', how='left')\n",
        "\n",
        "# Print columns to debug\n",
        "print(\"Train columns after metadata merges:\", train.columns.tolist())\n",
        "print(\"Test columns after metadata merges:\", test.columns.tolist())\n",
        "\n",
        "# Encode\n",
        "# Dynamically determine categorical column names, accounting for potential suffixes from merges\n",
        "cat_cols = []\n",
        "original_cat_names = ['customer_category', 'customer_status', 'grade_name', 'unit_name']\n",
        "\n",
        "for name in original_cat_names:\n",
        "    if f'{name}_x' in train.columns: # Check for _x suffix first\n",
        "        cat_cols.append(f'{name}_x')\n",
        "    elif name in train.columns: # Fallback to original name if no suffix\n",
        "        cat_cols.append(name)\n",
        "    else:\n",
        "        print(f\"Warning: Categorical column '{name}' (or '{name}_x') not found in train. Skipping.\")\n",
        "\n",
        "print(\"Categorical columns for encoding:\", cat_cols)\n",
        "\n",
        "le = LabelEncoder()\n",
        "for col in cat_cols:\n",
        "    train[col] = train[col].astype(str).fillna('UNKNOWN')\n",
        "    test[col] = test[col].astype(str).fillna('UNKNOWN')\n",
        "\n",
        "    combined = pd.concat([train[col], test[col]])\n",
        "    le.fit(combined)\n",
        "    train[col] = le.transform(train[col])\n",
        "    test[col] = le.transform(test[col])\n",
        "\n",
        "# Fill NaNs for numerical features that might have been created or merged\n",
        "# Ensure these are only numerical and exist\n",
        "num_cols = ['lag1', 'lag2', 'roll_mean_4', 'global_lag1', 'global_roll_4']\n",
        "for col in num_cols:\n",
        "    if col in train.columns:\n",
        "        train[col] = train[col].fillna(0)\n",
        "    if col in test.columns:\n",
        "        test[col] = test[col].fillna(0)\n",
        "\n",
        "# --- 5. Target Generation (Safe Mode) ---\n",
        "print(\"Generating Targets...\")\n",
        "# We generate targets on Train ONLY, using the sort order\n",
        "train = train.sort_values(['customer_id', 'product_unit_variant_id', 'week_start'])\n",
        "grp_train = train.groupby(['customer_id', 'product_unit_variant_id'])['qty_this_week']\n",
        "\n",
        "train['target_qty_1w'] = grp_train.shift(-1).fillna(0)\n",
        "train['target_qty_2w'] = grp_train.shift(-2).fillna(0)\n",
        "train['target_buy_1w'] = (train['target_qty_1w'] > 0).astype(int)\n",
        "train['target_buy_2w'] = (train['target_qty_2w'] > 0).astype(int)\n",
        "\n",
        "# --- 6. Modeling ---\n",
        "print(\"Training Models...\")\n",
        "features = feature_cols + cat_cols\n",
        "\n",
        "# Define LightGBM Params (Tweedie + Optimization)\n",
        "clf_params = {'n_estimators': 2000, 'learning_rate': 0.02, 'random_state': 42, 'verbose': -1}\n",
        "reg_params = {'objective': 'tweedie', 'tweedie_variance_power': 1.5, 'n_estimators': 2000, 'learning_rate': 0.02, 'random_state': 42, 'verbose': -1}\n",
        "\n",
        "models = {}\n",
        "\n",
        "# Week 1\n",
        "print(\"  Week 1 Models...\")\n",
        "clf1 = lgb.LGBMClassifier(**clf_params)\n",
        "clf1.fit(train[features], train['target_buy_1w'])\n",
        "\n",
        "reg1 = lgb.LGBMRegressor(**reg_params)\n",
        "reg1.fit(train[features], train['target_qty_1w'])\n",
        "\n",
        "# Week 2\n",
        "print(\"  Week 2 Models...\")\n",
        "clf2 = lgb.LGBMClassifier(**clf_params)\n",
        "clf2.fit(train[features], train['target_buy_2w'])\n",
        "\n",
        "reg2 = lgb.LGBMRegressor(**reg_params)\n",
        "reg2.fit(train[features], train['target_qty_2w'])\n",
        "\n",
        "# --- 7. Submission (Guaranteed Alignment) ---\n",
        "print(\"Generating Submission...\")\n",
        "# We predict directly on 'test' dataframe which is still in the original order from Test.csv\n",
        "submission = test[['ID']].copy()\n",
        "\n",
        "p1 = clf1.predict_proba(test[features])[:, 1]\n",
        "q1 = reg1.predict(test[features])\n",
        "p2 = clf2.predict_proba(test[features])[:, 1]\n",
        "q2 = reg2.predict(test[features])\n",
        "\n",
        "submission['Target_purchase_next_1w'] = p1\n",
        "submission['Target_purchase_next_2w'] = p2\n",
        "\n",
        "# EXPECTED VALUE OPTIMIZATION (Qty * Prob)\n",
        "submission['Target_qty_next_1w'] = (q1 * p1).clip(min=0)\n",
        "submission['Target_qty_next_2w'] = (q2 * p2).clip(min=0)\n",
        "\n",
        "submission.to_csv('submission_logic_fixed.csv', index=False)\n",
        "print(\"Done! Upload 'submission_logic_fixed.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6T3HfMI0P7Ea",
        "outputId": "a48fa645-4f45-4c7b-cd93-f30f938c09d4"
      },
      "id": "6T3HfMI0P7Ea",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Generating Features...\n",
            "  Calculating Rolling Stats...\n",
            "  Calculating Global Trends...\n",
            "Merging Features back to Train/Test...\n",
            "Merging Metadata...\n",
            "Train columns after metadata merges: ['ID', 'customer_id', 'product_unit_variant_id', 'week_start', 'qty_this_week', 'num_orders_week', 'spend_this_week', 'purchased_this_week', 'product_id', 'grade_name_x', 'unit_name_x', 'product_grade_variant_id', 'selling_price', 'customer_category_x', 'customer_status_x', 'customer_created_at_x', 'Target_qty_next_1w', 'Target_purchase_next_1w', 'Target_qty_next_2w', 'Target_purchase_next_2w', 'lag1', 'lag2', 'roll_mean_4', 'global_lag1', 'global_roll_4', 'month', 'week_of_year', 'customer_category_y', 'customer_status_y', 'customer_created_at_y', 'product_name', 'product_grade_variant_sku', 'unit_name_y', 'grade_name_y', 'grade_active_status']\n",
            "Test columns after metadata merges: ['ID', 'customer_id', 'product_unit_variant_id', 'week_start', 'product_id', 'grade_name_x', 'unit_name_x', 'product_grade_variant_id', 'customer_category_x', 'customer_status_x', 'customer_created_at_x', 'lag1', 'lag2', 'roll_mean_4', 'global_lag1', 'global_roll_4', 'month', 'week_of_year', 'customer_category_y', 'customer_status_y', 'customer_created_at_y', 'product_name', 'product_grade_variant_sku', 'unit_name_y', 'grade_name_y', 'grade_active_status']\n",
            "Categorical columns for encoding: ['customer_category_x', 'customer_status_x', 'grade_name_x', 'unit_name_x']\n",
            "Generating Targets...\n",
            "Training Models...\n",
            "  Week 1 Models...\n",
            "  Week 2 Models...\n",
            "Generating Submission...\n",
            "Done! Upload 'submission_logic_fixed.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Farm To Feed Competition - Grandmaster Final Solution\n",
        "# Author: Kaggle GM Time-Series Retail Pipeline\n",
        "# ============================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "import gc\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# ============================================================\n",
        "# 1. LOAD DATA (SAFE MODE - NO DOWNCASTING)\n",
        "# ============================================================\n",
        "\n",
        "print(\"Loading data...\")\n",
        "\n",
        "train = pd.read_csv(\"Train.csv\")\n",
        "test = pd.read_csv(\"Test.csv\")\n",
        "customer = pd.read_csv(\"customer_data.csv\")\n",
        "sku = pd.read_csv(\"sku_data.csv\")\n",
        "\n",
        "train[\"week_start\"] = pd.to_datetime(train[\"week_start\"])\n",
        "test[\"week_start\"] = pd.to_datetime(test[\"week_start\"])\n",
        "customer[\"customer_created_at\"] = pd.to_datetime(customer[\"customer_created_at\"])\n",
        "\n",
        "# ============================================================\n",
        "# 2. UNIVERSAL DATAFRAME FOR FEATURE ENGINEERING\n",
        "# ============================================================\n",
        "\n",
        "print(\"Building universal dataframe...\")\n",
        "\n",
        "base_cols = [\"customer_id\", \"product_unit_variant_id\", \"week_start\", \"qty_this_week\"]\n",
        "\n",
        "temp_df = pd.concat(\n",
        "    [\n",
        "        train[base_cols],\n",
        "        test[[\"customer_id\", \"product_unit_variant_id\", \"week_start\"]]\n",
        "        .assign(qty_this_week=0.0),\n",
        "    ],\n",
        "    ignore_index=True,\n",
        ")\n",
        "\n",
        "temp_df = temp_df.sort_values(\n",
        "    [\"customer_id\", \"product_unit_variant_id\", \"week_start\"]\n",
        ").reset_index(drop=True)\n",
        "\n",
        "# ============================================================\n",
        "# 3. CUSTOMER-PRODUCT LAGS & ROLLING STATS\n",
        "# ============================================================\n",
        "\n",
        "print(\"Generating customer-product features...\")\n",
        "\n",
        "pair_grp = temp_df.groupby(\n",
        "    [\"customer_id\", \"product_unit_variant_id\"]\n",
        ")[\"qty_this_week\"]\n",
        "\n",
        "temp_df[\"lag1\"] = pair_grp.shift(1)\n",
        "temp_df[\"lag2\"] = pair_grp.shift(2)\n",
        "\n",
        "temp_df[\"roll_mean_4\"] = pair_grp.transform(\n",
        "    lambda x: x.shift(1).rolling(4).mean()\n",
        ")\n",
        "\n",
        "# Cold-start indicator\n",
        "temp_df[\"is_new_pair\"] = pair_grp.shift(1).isna().astype(int)\n",
        "\n",
        "# ============================================================\n",
        "# 4. PRODUCT-CUSTOMER RECENCY (WEEKS SINCE LAST PURCHASE)\n",
        "# ============================================================\n",
        "\n",
        "print(\"Calculating recency...\")\n",
        "\n",
        "def recency_weeks(x):\n",
        "    last = x.where(x > 0).shift(1)\n",
        "    return last.groupby(level=0).cumcount()\n",
        "\n",
        "temp_df[\"pair_recency\"] = pair_grp.transform(\n",
        "    lambda x: x.shift(1).notna().cumsum()\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# 5. CUSTOMER MOMENTUM (GLOBAL CUSTOMER BEHAVIOR)\n",
        "# ============================================================\n",
        "\n",
        "print(\"Generating customer momentum features...\")\n",
        "\n",
        "cust_grp = temp_df.groupby(\"customer_id\")[\"qty_this_week\"]\n",
        "\n",
        "temp_df[\"cust_lag1\"] = cust_grp.shift(1)\n",
        "temp_df[\"cust_roll_4\"] = cust_grp.transform(\n",
        "    lambda x: x.shift(1).rolling(4).mean()\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# 6. GLOBAL PRODUCT TRENDS (CRITICAL FIX: USE SUM)\n",
        "# ============================================================\n",
        "\n",
        "print(\"Generating global product trends...\")\n",
        "\n",
        "global_weekly = (\n",
        "    temp_df.groupby(\n",
        "        [\"product_unit_variant_id\", \"week_start\"]\n",
        "    )[\"qty_this_week\"]\n",
        "    .sum()\n",
        "    .reset_index()\n",
        "    .rename(columns={\"qty_this_week\": \"global_weekly_vol\"})\n",
        ")\n",
        "\n",
        "temp_df = temp_df.merge(\n",
        "    global_weekly,\n",
        "    on=[\"product_unit_variant_id\", \"week_start\"],\n",
        "    how=\"left\",\n",
        ")\n",
        "\n",
        "prod_grp = temp_df.groupby(\"product_unit_variant_id\")[\"global_weekly_vol\"]\n",
        "\n",
        "temp_df[\"global_lag1\"] = prod_grp.shift(1)\n",
        "temp_df[\"global_roll_4\"] = prod_grp.transform(\n",
        "    lambda x: x.shift(1).rolling(4).mean()\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# 7. SEASONALITY\n",
        "# ============================================================\n",
        "\n",
        "temp_df[\"month\"] = temp_df[\"week_start\"].dt.month.astype(int)\n",
        "temp_df[\"week_of_year\"] = (\n",
        "    temp_df[\"week_start\"].dt.isocalendar().week.astype(int)\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# 8. MERGE FEATURES BACK (ID-SAFE)\n",
        "# ============================================================\n",
        "\n",
        "print(\"Merging engineered features...\")\n",
        "\n",
        "feature_cols = [\n",
        "    \"lag1\", \"lag2\", \"roll_mean_4\",\n",
        "    \"cust_lag1\", \"cust_roll_4\",\n",
        "    \"global_lag1\", \"global_roll_4\",\n",
        "    \"is_new_pair\", \"pair_recency\",\n",
        "    \"month\", \"week_of_year\",\n",
        "]\n",
        "\n",
        "merge_cols = [\"customer_id\", \"product_unit_variant_id\", \"week_start\"]\n",
        "\n",
        "train = train.merge(\n",
        "    temp_df[merge_cols + feature_cols],\n",
        "    on=merge_cols,\n",
        "    how=\"left\",\n",
        ")\n",
        "\n",
        "test = test.merge(\n",
        "    temp_df[merge_cols + feature_cols],\n",
        "    on=merge_cols,\n",
        "    how=\"left\",\n",
        ")\n",
        "\n",
        "del temp_df, pair_grp, cust_grp, prod_grp, global_weekly\n",
        "gc.collect()\n",
        "\n",
        "# ============================================================\n",
        "# 9. MERGE METADATA & ENCODE CATEGORICALS\n",
        "# ============================================================\n",
        "\n",
        "print(\"Merging metadata...\")\n",
        "\n",
        "train = train.merge(customer, on=\"customer_id\", how=\"left\")\n",
        "train = train.merge(sku, on=\"product_unit_variant_id\", how=\"left\")\n",
        "\n",
        "test = test.merge(customer, on=\"customer_id\", how=\"left\")\n",
        "test = test.merge(sku, on=\"product_unit_variant_id\", how=\"left\")\n",
        "\n",
        "cat_candidates = [\n",
        "    \"customer_category\",\n",
        "    \"customer_status\",\n",
        "    \"grade_name\",\n",
        "    \"unit_name\",\n",
        "]\n",
        "\n",
        "cat_cols = []\n",
        "for c in cat_candidates:\n",
        "    if f\"{c}_x\" in train.columns:\n",
        "        cat_cols.append(f\"{c}_x\")\n",
        "    elif c in train.columns:\n",
        "        cat_cols.append(c)\n",
        "\n",
        "le = LabelEncoder()\n",
        "for col in cat_cols:\n",
        "    train[col] = train[col].astype(str).fillna(\"UNKNOWN\")\n",
        "    test[col] = test[col].astype(str).fillna(\"UNKNOWN\")\n",
        "    le.fit(pd.concat([train[col], test[col]]))\n",
        "    train[col] = le.transform(train[col])\n",
        "    test[col] = le.transform(test[col])\n",
        "\n",
        "# Fill numerical NaNs\n",
        "for col in feature_cols:\n",
        "    train[col] = train[col].fillna(0)\n",
        "    test[col] = test[col].fillna(0)\n",
        "\n",
        "# ============================================================\n",
        "# 10. TARGET GENERATION (NO ROW DROPPING)\n",
        "# ============================================================\n",
        "\n",
        "print(\"Generating targets...\")\n",
        "\n",
        "train = train.sort_values(\n",
        "    [\"customer_id\", \"product_unit_variant_id\", \"week_start\"]\n",
        ")\n",
        "\n",
        "grp = train.groupby(\n",
        "    [\"customer_id\", \"product_unit_variant_id\"]\n",
        ")[\"qty_this_week\"]\n",
        "\n",
        "train[\"target_qty_1w\"] = grp.shift(-1).fillna(0)\n",
        "train[\"target_qty_2w\"] = grp.shift(-2).fillna(0)\n",
        "\n",
        "train[\"target_buy_1w\"] = (train[\"target_qty_1w\"] > 0).astype(int)\n",
        "train[\"target_buy_2w\"] = (train[\"target_qty_2w\"] > 0).astype(int)\n",
        "\n",
        "# ============================================================\n",
        "# 11. MODEL TRAINING (4 MODELS - TUNED)\n",
        "# ============================================================\n",
        "\n",
        "print(\"Training models...\")\n",
        "\n",
        "features = feature_cols + cat_cols\n",
        "\n",
        "clf_params = dict(\n",
        "    n_estimators=2000,\n",
        "    learning_rate=0.02,\n",
        "    num_leaves=64,\n",
        "    feature_fraction=0.8,\n",
        "    bagging_fraction=0.8,\n",
        "    bagging_freq=1,\n",
        "    random_state=42,\n",
        "    verbose=-1,\n",
        ")\n",
        "\n",
        "reg1_params = dict(\n",
        "    objective=\"tweedie\",\n",
        "    tweedie_variance_power=1.3,\n",
        "    n_estimators=2000,\n",
        "    learning_rate=0.02,\n",
        "    num_leaves=64,\n",
        "    feature_fraction=0.8,\n",
        "    random_state=42,\n",
        "    verbose=-1,\n",
        ")\n",
        "\n",
        "reg2_params = dict(\n",
        "    objective=\"tweedie\",\n",
        "    tweedie_variance_power=1.6,\n",
        "    n_estimators=2000,\n",
        "    learning_rate=0.02,\n",
        "    num_leaves=64,\n",
        "    feature_fraction=0.8,\n",
        "    random_state=42,\n",
        "    verbose=-1,\n",
        ")\n",
        "\n",
        "clf1 = lgb.LGBMClassifier(**clf_params)\n",
        "clf2 = lgb.LGBMClassifier(**clf_params)\n",
        "\n",
        "reg1 = lgb.LGBMRegressor(**reg1_params)\n",
        "reg2 = lgb.LGBMRegressor(**reg2_params)\n",
        "\n",
        "clf1.fit(train[features], train[\"target_buy_1w\"])\n",
        "clf2.fit(train[features], train[\"target_buy_2w\"])\n",
        "\n",
        "reg1.fit(train[features], train[\"target_qty_1w\"])\n",
        "reg2.fit(train[features], train[\"target_qty_2w\"])\n",
        "\n",
        "# ============================================================\n",
        "# 12. SUBMISSION (EXPECTED VALUE OPTIMIZATION)\n",
        "# ============================================================\n",
        "\n",
        "print(\"Generating submission...\")\n",
        "\n",
        "submission = test[[\"ID\"]].copy()\n",
        "\n",
        "p1 = clf1.predict_proba(test[features])[:, 1]\n",
        "p2 = clf2.predict_proba(test[features])[:, 1]\n",
        "\n",
        "q1 = reg1.predict(test[features])\n",
        "q2 = reg2.predict(test[features])\n",
        "\n",
        "submission[\"Target_purchase_next_1w\"] = p1\n",
        "submission[\"Target_purchase_next_2w\"] = p2\n",
        "\n",
        "submission[\"Target_qty_next_1w\"] = (p1 * q1).clip(min=0)\n",
        "submission[\"Target_qty_next_2w\"] = (p2 * q2).clip(min=0)\n",
        "\n",
        "submission.to_csv(\"submission_gm_final.csv\", index=False)\n",
        "\n",
        "print(\"Done. File saved as submission_gm_final.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j23CGtzLfzec",
        "outputId": "8ed92d85-715b-46a4-f147-ec7a5e5b39c0"
      },
      "id": "j23CGtzLfzec",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Building universal dataframe...\n",
            "Generating customer-product features...\n",
            "Calculating recency...\n",
            "Generating customer momentum features...\n",
            "Generating global product trends...\n",
            "Merging engineered features...\n",
            "Merging metadata...\n",
            "Generating targets...\n",
            "Training models...\n",
            "Generating submission...\n",
            "Done. File saved as submission_gm_final.csv\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}