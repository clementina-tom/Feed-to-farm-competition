{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory-Efficient ML Pipeline for Farm To Feed Dataset (Colab-friendly)\n",
    "\n",
    "This notebook is a Colab-ready version of pipeline_redo.ipynb. It downloads the required CSVs from the repository when possible and falls back to interactive upload. It also installs LightGBM and contains the same memory-efficient pipeline with downcasting and LightGBM models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install dependencies\n",
    "!pip install -q lightgbm\n",
    "# requests is in the runtime by default; ipywidgets not required in Colab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "from sklearn.metrics import roc_auc_score, mean_absolute_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os, requests, io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Data-loading: attempt to download from the repo at the provided commit, otherwise fall back to manual upload\n",
    "print('Attempting to download CSVs from GitHub raw...')\n",
    "base = 'https://raw.githubusercontent.com/clementina-tom/Feed-to-farm-competition/6fdcfec5fb497555503a589050ef8876818297dc/'\n",
    "files = ['Train.csv', 'Test.csv', 'customer_data.csv', 'sku_data.csv']\n",
    "for fname in files:\n",
    "    url = base + fname\n",
    "    out = fname\n",
    "    try:\n",
    "        r = requests.get(url, timeout=15)\n",
    "        r.raise_for_status()\n",
    "        open(out, 'wb').write(r.content)\n",
    "        print('Downloaded', out)\n",
    "    except Exception as e:\n",
    "        print('Could not download', out, '-', e)\n",
    "# If any files missing, fall back to interactive upload in Colab\n",
    "missing = [f for f in files if not os.path.exists(f)]\n",
    "if missing:\n",
    "    print('Missing files:', missing)\n",
    "    try:\n",
    "        from google.colab import files as colab_files\n",
    "        uploaded = colab_files.upload()\n",
    "        for k, v in uploaded.items():\n",
    "            open(k, 'wb').write(v)\n",
    "            print('Uploaded', k)\n",
    "    except Exception as e:\n",
    "        print('Colab upload unavailable or skipped:', e)\n",
    "\n",
    "# Read CSVs into dataframes\n",
    "train = pd.read_csv('Train.csv')\n",
    "test = pd.read_csv('Test.csv')\n",
    "customer = pd.read_csv('customer_data.csv')\n",
    "sku = pd.read_csv('sku_data.csv')\n",
    "print('Shapes:', train.shape, test.shape, customer.shape, sku.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Downcast helper and preprocessing\n",
    "def downcast(df):\n",
    "    for col in df.select_dtypes(include=['float64']).columns:\n",
    "        df[col] = df[col].astype('float32')\n",
    "    for col in df.select_dtypes(include=['int64']).columns:\n",
    "        max_val = df[col].max()\n",
    "        if pd.isna(max_val):\n",
    "            continue\n",
    "        if max_val < 2**7:\n",
    "            df[col] = df[col].astype('int8')\n",
    "        elif max_val < 2**15:\n",
    "            df[col] = df[col].astype('int16')\n",
    "        else:\n",
    "            df[col] = df[col].astype('int32')\n",
    "    return df\n",
    "\n",
    "train = downcast(train)\n",
    "test = downcast(test)\n",
    "customer = downcast(customer)\n",
    "sku = downcast(sku)\n",
    "\n",
    "train['week_start'] = pd.to_datetime(train['week_start'])\n",
    "test['week_start'] = pd.to_datetime(test['week_start'])\n",
    "customer['customer_created_at'] = pd.to_datetime(customer['customer_created_at'])\n",
    "gc.collect()\n",
    "print('Downcasting and datetime conversion done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Merge metadata\n",
    "train = train.merge(customer, on='customer_id', how='left', suffixes=('', '_cust'))\n",
    "train = train.merge(sku, on='product_unit_variant_id', how='left', suffixes=('', '_sku'))\n",
    "test = test.merge(customer, on='customer_id', how='left', suffixes=('', '_cust'))\n",
    "test = test.merge(sku, on='product_unit_variant_id', how='left', suffixes=('', '_sku'))\n",
    "gc.collect()\n",
    "print('Merged customer and sku metadata')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Feature engineering using a memory-friendly approach\n",
    "full_df = pd.concat([train[['customer_id', 'product_unit_variant_id', 'week_start', 'qty_this_week']],\n",
    "                     test[['customer_id', 'product_unit_variant_id', 'week_start']].assign(qty_this_week=0.0)])\n",
    "full_df = downcast(full_df)\n",
    "full_df = full_df.sort_values(['customer_id', 'product_unit_variant_id', 'week_start'])\n",
    "\n",
    "print('Creating lag & rolling features...')\n",
    "full_df['lag1_qty'] = full_df.groupby(['customer_id', 'product_unit_variant_id'])['qty_this_week'].shift(1)\n",
    "full_df['lag2_qty'] = full_df.groupby(['customer_id', 'product_unit_variant_id'])['qty_this_week'].shift(2)\n",
    "full_df['lag3_qty'] = full_df.groupby(['customer_id', 'product_unit_variant_id'])['qty_this_week'].shift(3)\n",
    "full_df['rolling_mean_4'] = full_df.groupby(['customer_id', 'product_unit_variant_id'])['qty_this_week'].rolling(4).mean().reset_index(0, drop=True)\n",
    "full_df['rolling_max_4'] = full_df.groupby(['customer_id', 'product_unit_variant_id'])['qty_this_week'].rolling(4).max().reset_index(0, drop=True)\n",
    "\n",
    "full_df[['lag1_qty', 'lag2_qty', 'lag3_qty', 'rolling_mean_4', 'rolling_max_4']] = full_df[['lag1_qty', 'lag2_qty', 'lag3_qty', 'rolling_mean_4', 'rolling_max_4']].fillna(0)\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Merge features back into train/test\n",
    "feature_cols = ['lag1_qty', 'lag2_qty', 'lag3_qty', 'rolling_mean_4', 'rolling_max_4']\n",
    "train = train.merge(full_df[['customer_id', 'product_unit_variant_id', 'week_start'] + feature_cols], \n",
    "                    on=['customer_id', 'product_unit_variant_id', 'week_start'], how='left')\n",
    "test = test.merge(full_df[['customer_id', 'product_unit_variant_id', 'week_start'] + feature_cols], \n",
    "                  on=['customer_id', 'product_unit_variant_id', 'week_start'], how='left')\n",
    "del full_df\n",
    "gc.collect()\n",
    "print('Features merged')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Target generation\n",
    "print('Creating targets...')\n",
    "train = train.sort_values(['customer_id', 'product_unit_variant_id', 'week_start'])\n",
    "train['target_purchase_1w'] = (train.groupby(['customer_id', 'product_unit_variant_id'])['qty_this_week'].shift(-1) > 0).astype(int)\n",
    "train['target_qty_1w'] = train.groupby(['customer_id', 'product_unit_variant_id'])['qty_this_week'].shift(-1).fillna(0)\n",
    "train['target_purchase_2w'] = (train.groupby(['customer_id', 'product_unit_variant_id'])['qty_this_week'].shift(-2) > 0).astype(int)\n",
    "train['target_qty_2w'] = train.groupby(['customer_id', 'product_unit_variant_id'])['qty_this_week'].shift(-2).fillna(0)\n",
    "train = train.dropna(subset=['target_purchase_1w'])\n",
    "gc.collect()\n",
    "print('Targets created')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Encode categorical columns robustly\n",
    "cat_cols = ['customer_category', 'customer_status', 'grade_name', 'unit_name']\n",
    "le = LabelEncoder()\n",
    "for col in cat_cols:\n",
    "    if col in train.columns and col in test.columns:\n",
    "        combined = pd.concat([train[col].astype(str), test[col].astype(str)])\n",
    "        le.fit(combined)\n",
    "        train[col] = le.transform(train[col].astype(str))\n",
    "        test[col] = le.transform(test[col].astype(str))\n",
    "\n",
    "features = ['selling_price', 'customer_category', 'customer_status', 'grade_name', 'unit_name'] + feature_cols\n",
    "print('Categorical encoding done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Modeling\n",
    "print('Training models...')\n",
    "weeks = sorted(train['week_start'].unique())\n",
    "val_weeks = weeks[-2:] if len(weeks) >= 2 else weeks\n",
    "val_mask = train['week_start'].isin(val_weeks)\n",
    "models = {}\n",
    "targets = ['target_purchase_1w', 'target_qty_1w', 'target_purchase_2w', 'target_qty_2w']\n",
    "for target in targets:\n",
    "    print(f'Training {target}...')\n",
    "    X = train[features].fillna(0)\n",
    "    y = train[target]\n",
    "    X_train, X_val = X[~val_mask], X[val_mask]\n",
    "    y_train, y_val = y[~val_mask], y[val_mask]\n",
    "    if 'purchase' in target:\n",
    "        model = lgb.LGBMClassifier(n_estimators=1000, early_stopping_rounds=50, verbose=-1)\n",
    "        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], eval_metric='auc')\n",
    "        pred_val = model.predict_proba(X_val)[:, 1]\n",
    "        auc = roc_auc_score(y_val, pred_val)\n",
    "        print(f'AUC for {target}: {auc}')\n",
    "    else:\n",
    "        model = lgb.LGBMRegressor(n_estimators=1000, early_stopping_rounds=50, verbose=-1)\n",
    "        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], eval_metric='mae')\n",
    "        pred_val = model.predict(X_val)\n",
    "        mae = mean_absolute_error(y_val, pred_val)\n",
    "        print(f'MAE for {target}: {mae}')\n",
    "    models[target] = model\n",
    "    gc.collect()\n",
    "print('Modeling complete')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate predictions and save submission.csv\n",
    "print('Generating predictions...')\n",
    "test_features = test[features].fillna(0)\n",
    "test['Target_purchase_next_1w'] = models['target_purchase_1w'].predict_proba(test_features)[:, 1]\n",
    "test['Target_qty_next_1w'] = models['target_qty_1w'].predict(test_features)\n",
    "test['Target_purchase_next_2w'] = models['target_purchase_2w'].predict_proba(test_features)[:, 1]\n",
    "test['Target_qty_next_2w'] = models['target_qty_2w'].predict(test_features)\n",
    "submission = test[['ID', 'Target_purchase_next_1w', 'Target_qty_next_1w', 'Target_purchase_next_2w', 'Target_qty_next_2w']]\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print('submission.csv saved')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
